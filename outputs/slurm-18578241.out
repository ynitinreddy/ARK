/home/nyarava/miniconda3/envs/yolo/lib/python3.12/site-packages/albumentations/__init__.py:13: UserWarning: A new version of Albumentations is available: 1.4.15 (you have 1.4.14). Upgrade using: pip install -U albumentations. To disable automatic update checks, set the environment variable NO_ALBUMENTATIONS_UPDATE to 1.
  check_for_updates()
/home/nyarava/miniconda3/envs/yolo/lib/python3.12/site-packages/torch/functional.py:507: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3549.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
{'GPU': None, 'model_name': 'swin_base', 'init': 'ark', 'pretrained_weights': '/home/nyarava/ARK/Ark/ark6_teacher_ep200_swinb_projector1376_mlp.pth.tar', 'num_class': 14, 'data_set': 'CheXpert', 'normalization': 'imagenet', 'img_size': 224, 'img_depth': 3, 'data_dir': '/scratch/nyarava/hey.zip/chexpertchestxrays-u20210408/', 'train_list': '/scratch/nyarava/hey.zip/chexpertchestxrays-u20210408/CheXpert-v1.0/train.csv', 'val_list': '/scratch/nyarava/hey.zip/chexpertchestxrays-u20210408/CheXpert-v1.0/valid.csv', 'test_list': '/scratch/nyarava/hey.zip/chexpertchestxrays-u20210408/CheXpert-v1.0/test_labels.csv', 'mode': 'train', 'batch_size': 64, 'epochs': 8, 'exp_name': '', 'opt': 'adamw', 'opt_eps': 1e-08, 'opt_betas': None, 'clip_grad': None, 'momentum': 0.9, 'weight_decay': 0.0001, 'sched': 'cosine', 'lr': 0.001, 'lr_noise': None, 'lr_noise_pct': 0.67, 'lr_noise_std': 1.0, 'warmup_lr': 1e-06, 'min_lr': 1e-05, 'decay_epochs': 30, 'warmup_epochs': 3, 'cooldown_epochs': 10, 'decay_rate': 0.5, 'patience': 10, 'early_stop': True, 'num_trial': 1, 'start_index': 0, 'clean': False, 'resume': False, 'workers': 8, 'print_freq': 50, 'test_augment': True, 'anno_percent': 100, 'device': 'cuda', 'activate': 'Sigmoid', 'uncertain_label': 'LSR-Ones', 'unknown_label': 0}
start training....
run: 1
Creating model...
Creating model from pretrained weights: /home/nyarava/ARK/Ark/ark6_teacher_ep200_swinb_projector1376_mlp.pth.tar
Removing key head.weight from pretrained checkpoint
Removing key head.bias from pretrained checkpoint
Loaded with msg: _IncompatibleKeys(missing_keys=['head.weight', 'head.bias'], unexpected_keys=['projector.0.weight', 'projector.0.bias', 'projector.2.weight', 'projector.2.bias', 'omni_heads.0.weight', 'omni_heads.0.bias', 'omni_heads.1.weight', 'omni_heads.1.bias', 'omni_heads.2.weight', 'omni_heads.2.bias', 'omni_heads.3.weight', 'omni_heads.3.bias', 'omni_heads.4.weight', 'omni_heads.4.bias', 'omni_heads.5.weight', 'omni_heads.5.bias'])
SwinTransformer(
  (patch_embed): PatchEmbed(
    (proj): Conv2d(3, 128, kernel_size=(4, 4), stride=(4, 4))
    (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
  )
  (pos_drop): Dropout(p=0.0, inplace=False)
  (layers): Sequential(
    (0): BasicLayer(
      dim=128, input_resolution=(56, 56), depth=2
      (blocks): ModuleList(
        (0): SwinTransformerBlock(
          (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            (qkv): Linear(in_features=128, out_features=384, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=128, out_features=128, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): Identity()
          (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=128, out_features=512, bias=True)
            (act): GELU(approximate='none')
            (drop1): Dropout(p=0.0, inplace=False)
            (fc2): Linear(in_features=512, out_features=128, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
        )
        (1): SwinTransformerBlock(
          (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            (qkv): Linear(in_features=128, out_features=384, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=128, out_features=128, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=128, out_features=512, bias=True)
            (act): GELU(approximate='none')
            (drop1): Dropout(p=0.0, inplace=False)
            (fc2): Linear(in_features=512, out_features=128, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (downsample): PatchMerging(
        input_resolution=(56, 56), dim=128
        (reduction): Linear(in_features=512, out_features=256, bias=False)
        (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
    )
    (1): BasicLayer(
      dim=256, input_resolution=(28, 28), depth=2
      (blocks): ModuleList(
        (0-1): 2 x SwinTransformerBlock(
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            (qkv): Linear(in_features=256, out_features=768, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=256, out_features=1024, bias=True)
            (act): GELU(approximate='none')
            (drop1): Dropout(p=0.0, inplace=False)
            (fc2): Linear(in_features=1024, out_features=256, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (downsample): PatchMerging(
        input_resolution=(28, 28), dim=256
        (reduction): Linear(in_features=1024, out_features=512, bias=False)
        (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
    )
    (2): BasicLayer(
      dim=512, input_resolution=(14, 14), depth=18
      (blocks): ModuleList(
        (0-17): 18 x SwinTransformerBlock(
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU(approximate='none')
            (drop1): Dropout(p=0.0, inplace=False)
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (downsample): PatchMerging(
        input_resolution=(14, 14), dim=512
        (reduction): Linear(in_features=2048, out_features=1024, bias=False)
        (norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
    )
    (3): BasicLayer(
      dim=1024, input_resolution=(7, 7), depth=2
      (blocks): ModuleList(
        (0-1): 2 x SwinTransformerBlock(
          (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            (qkv): Linear(in_features=1024, out_features=3072, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=1024, out_features=1024, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=1024, out_features=4096, bias=True)
            (act): GELU(approximate='none')
            (drop1): Dropout(p=0.0, inplace=False)
            (fc2): Linear(in_features=4096, out_features=1024, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
        )
      )
    )
  )
  (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
  (avgpool): AdaptiveAvgPool1d(output_size=1)
  (head): Linear(in_features=1024, out_features=14, bias=True)
)
Epoch: [0][   0/3491]	Time 25.505 (25.505)	Loss 7.4578e-01 (7.4578e-01)
Epoch: [0][  50/3491]	Time  0.611 ( 2.417)	Loss 5.6674e-01 (6.5064e-01)
Epoch: [0][ 100/3491]	Time  0.491 ( 2.159)	Loss 4.6273e-01 (5.8097e-01)
Epoch: [0][ 150/3491]	Time  0.486 ( 2.058)	Loss 4.2735e-01 (5.3431e-01)
Epoch: [0][ 200/3491]	Time  9.172 ( 2.069)	Loss 4.1441e-01 (5.0397e-01)
Epoch: [0][ 250/3491]	Time  5.407 ( 2.060)	Loss 4.1683e-01 (4.8380e-01)
Epoch: [0][ 300/3491]	Time  0.486 ( 2.036)	Loss 3.7192e-01 (4.6847e-01)
Epoch: [0][ 350/3491]	Time  0.486 ( 2.030)	Loss 3.8412e-01 (4.5678e-01)
Epoch: [0][ 400/3491]	Time  0.490 ( 2.010)	Loss 3.9729e-01 (4.4801e-01)
Epoch: [0][ 450/3491]	Time  0.491 ( 2.014)	Loss 3.9010e-01 (4.4065e-01)
Epoch: [0][ 500/3491]	Time  0.490 ( 2.012)	Loss 3.6549e-01 (4.3408e-01)
Epoch: [0][ 550/3491]	Time  0.486 ( 2.010)	Loss 3.7290e-01 (4.2872e-01)
Epoch: [0][ 600/3491]	Time  0.491 ( 1.996)	Loss 3.9084e-01 (4.2411e-01)
Epoch: [0][ 650/3491]	Time  8.554 ( 1.996)	Loss 3.7070e-01 (4.2002e-01)
Epoch: [0][ 700/3491]	Time  0.491 ( 1.991)	Loss 3.6263e-01 (4.1638e-01)
Epoch: [0][ 750/3491]	Time  0.486 ( 1.985)	Loss 3.8201e-01 (4.1323e-01)
Epoch: [0][ 800/3491]	Time  0.488 ( 1.982)	Loss 3.7823e-01 (4.1015e-01)
Epoch: [0][ 850/3491]	Time 10.986 ( 1.985)	Loss 3.6525e-01 (4.0721e-01)
Epoch: [0][ 900/3491]	Time  0.486 ( 1.977)	Loss 3.3663e-01 (4.0476e-01)
Epoch: [0][ 950/3491]	Time  0.486 ( 1.973)	Loss 3.4350e-01 (4.0221e-01)
Epoch: [0][1000/3491]	Time  0.488 ( 1.981)	Loss 3.4080e-01 (4.0015e-01)
Epoch: [0][1050/3491]	Time  0.490 ( 1.973)	Loss 3.5780e-01 (3.9820e-01)
Epoch: [0][1100/3491]	Time  0.488 ( 1.965)	Loss 3.7121e-01 (3.9615e-01)
Epoch: [0][1150/3491]	Time  0.489 ( 1.966)	Loss 3.7264e-01 (3.9448e-01)
Epoch: [0][1200/3491]	Time  0.490 ( 1.978)	Loss 3.7227e-01 (3.9292e-01)
Epoch: [0][1250/3491]	Time  0.489 ( 1.991)	Loss 3.7231e-01 (3.9148e-01)
Epoch: [0][1300/3491]	Time  0.486 ( 2.004)	Loss 3.9391e-01 (3.9018e-01)
Epoch: [0][1350/3491]	Time  0.488 ( 2.007)	Loss 3.6400e-01 (3.8869e-01)
Epoch: [0][1400/3491]	Time  0.491 ( 2.022)	Loss 3.6272e-01 (3.8748e-01)
Epoch: [0][1450/3491]	Time  0.488 ( 2.036)	Loss 3.5331e-01 (3.8629e-01)
Epoch: [0][1500/3491]	Time  0.486 ( 2.046)	Loss 3.4382e-01 (3.8512e-01)
Epoch: [0][1550/3491]	Time  0.490 ( 2.053)	Loss 3.5690e-01 (3.8407e-01)
Epoch: [0][1600/3491]	Time  0.487 ( 2.069)	Loss 3.7179e-01 (3.8303e-01)
Epoch: [0][1650/3491]	Time  0.490 ( 2.071)	Loss 3.4280e-01 (3.8214e-01)
Epoch: [0][1700/3491]	Time  0.489 ( 2.073)	Loss 3.0347e-01 (3.8140e-01)
Epoch: [0][1750/3491]	Time  0.489 ( 2.070)	Loss 3.2397e-01 (3.8051e-01)
Epoch: [0][1800/3491]	Time  0.489 ( 2.072)	Loss 3.4536e-01 (3.7979e-01)
Epoch: [0][1850/3491]	Time  0.821 ( 2.068)	Loss 3.4240e-01 (3.7897e-01)
Epoch: [0][1900/3491]	Time  0.488 ( 2.069)	Loss 3.5654e-01 (3.7832e-01)
Epoch: [0][1950/3491]	Time  0.490 ( 2.071)	Loss 3.3765e-01 (3.7766e-01)
Epoch: [0][2000/3491]	Time  0.486 ( 2.074)	Loss 3.5918e-01 (3.7698e-01)
Epoch: [0][2050/3491]	Time  0.486 ( 2.075)	Loss 3.6132e-01 (3.7634e-01)
Epoch: [0][2100/3491]	Time  0.488 ( 2.069)	Loss 3.2581e-01 (3.7567e-01)
Epoch: [0][2150/3491]	Time  0.486 ( 2.069)	Loss 3.5304e-01 (3.7510e-01)
Epoch: [0][2200/3491]	Time  0.487 ( 2.067)	Loss 3.7556e-01 (3.7459e-01)
Epoch: [0][2250/3491]	Time  0.486 ( 2.066)	Loss 3.4216e-01 (3.7405e-01)
Epoch: [0][2300/3491]	Time  0.486 ( 2.064)	Loss 3.0447e-01 (3.7347e-01)
Epoch: [0][2350/3491]	Time  0.486 ( 2.060)	Loss 3.3740e-01 (3.7292e-01)
Epoch: [0][2400/3491]	Time  0.489 ( 2.062)	Loss 3.3574e-01 (3.7236e-01)
Epoch: [0][2450/3491]	Time  0.489 ( 2.064)	Loss 3.5403e-01 (3.7191e-01)
Epoch: [0][2500/3491]	Time  0.491 ( 2.061)	Loss 3.4036e-01 (3.7142e-01)
Epoch: [0][2550/3491]	Time  8.518 ( 2.065)	Loss 3.3913e-01 (3.7094e-01)
Epoch: [0][2600/3491]	Time  0.489 ( 2.063)	Loss 3.4161e-01 (3.7047e-01)
Epoch: [0][2650/3491]	Time  0.490 ( 2.065)	Loss 3.4215e-01 (3.6994e-01)
Epoch: [0][2700/3491]	Time  0.486 ( 2.069)	Loss 3.2468e-01 (3.6951e-01)
Epoch: [0][2750/3491]	Time 11.057 ( 2.070)	Loss 3.4757e-01 (3.6908e-01)
Epoch: [0][2800/3491]	Time  0.485 ( 2.066)	Loss 3.4702e-01 (3.6866e-01)
Epoch: [0][2850/3491]	Time  0.486 ( 2.066)	Loss 3.3799e-01 (3.6825e-01)
Epoch: [0][2900/3491]	Time  0.485 ( 2.066)	Loss 3.7774e-01 (3.6789e-01)
Epoch: [0][2950/3491]	Time  0.490 ( 2.064)	Loss 3.4204e-01 (3.6752e-01)
Epoch: [0][3000/3491]	Time  0.486 ( 2.063)	Loss 3.6843e-01 (3.6714e-01)
Epoch: [0][3050/3491]	Time  0.488 ( 2.061)	Loss 3.4933e-01 (3.6676e-01)
Epoch: [0][3100/3491]	Time  0.489 ( 2.056)	Loss 3.4245e-01 (3.6635e-01)
Epoch: [0][3150/3491]	Time  0.486 ( 2.055)	Loss 3.3754e-01 (3.6604e-01)
Epoch: [0][3200/3491]	Time  0.489 ( 2.050)	Loss 3.4388e-01 (3.6575e-01)
Epoch: [0][3250/3491]	Time  6.961 ( 2.047)	Loss 3.4418e-01 (3.6547e-01)
Epoch: [0][3300/3491]	Time  0.490 ( 2.044)	Loss 3.5201e-01 (3.6519e-01)
Epoch: [0][3350/3491]	Time  0.486 ( 2.044)	Loss 3.2836e-01 (3.6489e-01)
Epoch: [0][3400/3491]	Time  0.487 ( 2.043)	Loss 3.4021e-01 (3.6458e-01)
Epoch: [0][3450/3491]	Time  0.486 ( 2.042)	Loss 3.8189e-01 (3.6434e-01)
Val: [0/4]	Time 13.579 (13.579)	Loss 2.8684e-01 (2.8684e-01)
Epoch 0000: val_loss improved from 1000000.00000 to 0.33484, saving model to ./Models/Classification/CheXpert/swin_base_ark/swin_base_ark_run_0
Epoch: [1][   0/3491]	Time 19.549 (19.549)	Loss 3.4527e-01 (3.4527e-01)
Epoch: [1][  50/3491]	Time  0.488 ( 2.357)	Loss 3.6028e-01 (3.6391e-01)
Epoch: [1][ 100/3491]	Time  0.491 ( 2.001)	Loss 3.5161e-01 (3.6119e-01)
Epoch: [1][ 150/3491]	Time  0.491 ( 1.901)	Loss 3.5592e-01 (3.5799e-01)
Epoch: [1][ 200/3491]	Time 10.403 ( 1.884)	Loss 3.7084e-01 (3.5633e-01)
Epoch: [1][ 250/3491]	Time  0.489 ( 1.864)	Loss 3.3333e-01 (3.5519e-01)
Epoch: [1][ 300/3491]	Time  0.486 ( 1.842)	Loss 3.5885e-01 (3.5457e-01)
Epoch: [1][ 350/3491]	Time  0.490 ( 1.862)	Loss 3.5803e-01 (3.5372e-01)
Epoch: [1][ 400/3491]	Time  0.490 ( 1.874)	Loss 3.6273e-01 (3.5272e-01)
Epoch: [1][ 450/3491]	Time  0.490 ( 1.893)	Loss 3.9341e-01 (3.5222e-01)
Epoch: [1][ 500/3491]	Time  0.486 ( 1.886)	Loss 3.1753e-01 (3.5149e-01)
Epoch: [1][ 550/3491]	Time  0.486 ( 1.872)	Loss 3.4596e-01 (3.5078e-01)
Epoch: [1][ 600/3491]	Time  0.486 ( 1.864)	Loss 3.0603e-01 (3.5036e-01)
Epoch: [1][ 650/3491]	Time  0.486 ( 1.870)	Loss 3.5261e-01 (3.4988e-01)
Epoch: [1][ 700/3491]	Time  0.486 ( 1.861)	Loss 3.6986e-01 (3.4936e-01)
Epoch: [1][ 750/3491]	Time  0.490 ( 1.853)	Loss 3.5974e-01 (3.4895e-01)
Epoch: [1][ 800/3491]	Time 10.446 ( 1.862)	Loss 3.2249e-01 (3.4869e-01)
Epoch: [1][ 850/3491]	Time  0.490 ( 1.851)	Loss 3.5809e-01 (3.4833e-01)
Epoch: [1][ 900/3491]	Time  0.490 ( 1.842)	Loss 3.4704e-01 (3.4819e-01)
Epoch: [1][ 950/3491]	Time  0.487 ( 1.832)	Loss 3.7719e-01 (3.4793e-01)
Epoch: [1][1000/3491]	Time 10.390 ( 1.834)	Loss 3.3428e-01 (3.4776e-01)
Epoch: [1][1050/3491]	Time  0.489 ( 1.822)	Loss 3.5205e-01 (3.4742e-01)
Epoch: [1][1100/3491]	Time  0.486 ( 1.812)	Loss 3.7357e-01 (3.4719e-01)
Epoch: [1][1150/3491]	Time  0.486 ( 1.802)	Loss 3.6101e-01 (3.4689e-01)
Epoch: [1][1200/3491]	Time  8.606 ( 1.798)	Loss 3.3306e-01 (3.4668e-01)
Epoch: [1][1250/3491]	Time  0.486 ( 1.788)	Loss 3.4666e-01 (3.4664e-01)
Epoch: [1][1300/3491]	Time  0.486 ( 1.779)	Loss 4.0425e-01 (3.4645e-01)
Epoch: [1][1350/3491]	Time  0.486 ( 1.772)	Loss 3.2083e-01 (3.4626e-01)
Epoch: [1][1400/3491]	Time 10.618 ( 1.773)	Loss 3.6290e-01 (3.4621e-01)
Epoch: [1][1450/3491]	Time  0.486 ( 1.765)	Loss 3.4145e-01 (3.4606e-01)
Epoch: [1][1500/3491]	Time  0.490 ( 1.757)	Loss 3.4158e-01 (3.4597e-01)
Epoch: [1][1550/3491]	Time  0.490 ( 1.752)	Loss 3.5227e-01 (3.4585e-01)
Epoch: [1][1600/3491]	Time  6.901 ( 1.752)	Loss 3.2662e-01 (3.4569e-01)
Epoch: [1][1650/3491]	Time  0.486 ( 1.747)	Loss 3.6182e-01 (3.4569e-01)
Epoch: [1][1700/3491]	Time  0.490 ( 1.741)	Loss 3.1633e-01 (3.4563e-01)
Epoch: [1][1750/3491]	Time  0.490 ( 1.737)	Loss 3.2529e-01 (3.4551e-01)
Epoch: [1][1800/3491]	Time  5.640 ( 1.737)	Loss 3.4682e-01 (3.4549e-01)
Epoch: [1][1850/3491]	Time  0.491 ( 1.731)	Loss 3.4054e-01 (3.4530e-01)
Epoch: [1][1900/3491]	Time  0.489 ( 1.729)	Loss 3.4791e-01 (3.4523e-01)
Epoch: [1][1950/3491]	Time  0.490 ( 1.733)	Loss 3.5005e-01 (3.4510e-01)
Epoch: [1][2000/3491]	Time  8.133 ( 1.737)	Loss 3.5264e-01 (3.4517e-01)
Epoch: [1][2050/3491]	Time  0.486 ( 1.739)	Loss 3.1132e-01 (3.4510e-01)
Epoch: [1][2100/3491]	Time  0.486 ( 1.737)	Loss 3.3791e-01 (3.4509e-01)
Epoch: [1][2150/3491]	Time  2.495 ( 1.736)	Loss 3.5555e-01 (3.4506e-01)
Epoch: [1][2200/3491]	Time  6.220 ( 1.738)	Loss 3.4255e-01 (3.4498e-01)
Epoch: [1][2250/3491]	Time  0.490 ( 1.735)	Loss 3.2475e-01 (3.4489e-01)
Epoch: [1][2300/3491]	Time  0.489 ( 1.730)	Loss 3.4698e-01 (3.4485e-01)
Epoch: [1][2350/3491]	Time  1.253 ( 1.726)	Loss 3.2111e-01 (3.4478e-01)
Epoch: [1][2400/3491]	Time  6.912 ( 1.725)	Loss 2.8908e-01 (3.4482e-01)
Epoch: [1][2450/3491]	Time  0.485 ( 1.720)	Loss 3.8644e-01 (3.4475e-01)
Epoch: [1][2500/3491]	Time  0.486 ( 1.720)	Loss 3.4693e-01 (3.4466e-01)
Epoch: [1][2550/3491]	Time  1.097 ( 1.717)	Loss 3.6114e-01 (3.4459e-01)
Epoch: [1][2600/3491]	Time  7.367 ( 1.718)	Loss 3.2162e-01 (3.4463e-01)
Epoch: [1][2650/3491]	Time  0.490 ( 1.716)	Loss 3.6019e-01 (3.4456e-01)
Epoch: [1][2700/3491]	Time  0.490 ( 1.713)	Loss 3.1673e-01 (3.4444e-01)
Epoch: [1][2750/3491]	Time  4.463 ( 1.711)	Loss 3.2265e-01 (3.4436e-01)
Epoch: [1][2800/3491]	Time  3.500 ( 1.709)	Loss 3.4385e-01 (3.4432e-01)
Epoch: [1][2850/3491]	Time  0.487 ( 1.705)	Loss 3.2780e-01 (3.4428e-01)
Epoch: [1][2900/3491]	Time  0.490 ( 1.702)	Loss 3.7074e-01 (3.4415e-01)
Epoch: [1][2950/3491]	Time  1.097 ( 1.699)	Loss 3.7130e-01 (3.4399e-01)
Epoch: [1][3000/3491]	Time  8.118 ( 1.698)	Loss 3.4552e-01 (3.4390e-01)
Epoch: [1][3050/3491]	Time  0.486 ( 1.694)	Loss 3.3199e-01 (3.4386e-01)
Epoch: [1][3100/3491]	Time  0.490 ( 1.692)	Loss 3.3616e-01 (3.4381e-01)
Epoch: [1][3150/3491]	Time  5.651 ( 1.691)	Loss 3.4716e-01 (3.4377e-01)
Epoch: [1][3200/3491]	Time  3.668 ( 1.689)	Loss 3.4756e-01 (3.4369e-01)
Epoch: [1][3250/3491]	Time  0.488 ( 1.687)	Loss 3.3521e-01 (3.4365e-01)
Epoch: [1][3300/3491]	Time  0.486 ( 1.686)	Loss 3.3529e-01 (3.4363e-01)
Epoch: [1][3350/3491]	Time  0.486 ( 1.684)	Loss 3.5160e-01 (3.4358e-01)
Epoch: [1][3400/3491]	Time  0.486 ( 1.685)	Loss 3.4403e-01 (3.4357e-01)
Epoch: [1][3450/3491]	Time  0.838 ( 1.688)	Loss 3.4219e-01 (3.4352e-01)
Val: [0/4]	Time 12.687 (12.687)	Loss 2.8233e-01 (2.8233e-01)
Epoch 0001: val_loss did not improve from 0.33484 
Epoch: [2][   0/3491]	Time 11.052 (11.052)	Loss 3.7610e-01 (3.7610e-01)
Epoch: [2][  50/3491]	Time  0.485 ( 1.483)	Loss 3.4694e-01 (3.4097e-01)
Epoch: [2][ 100/3491]	Time  0.489 ( 1.417)	Loss 3.4624e-01 (3.3928e-01)
Epoch: [2][ 150/3491]	Time  0.491 ( 1.384)	Loss 3.2165e-01 (3.3957e-01)
Epoch: [2][ 200/3491]	Time  3.699 ( 1.403)	Loss 4.0663e-01 (3.4038e-01)
Epoch: [2][ 250/3491]	Time  0.489 ( 1.396)	Loss 3.5385e-01 (3.3949e-01)
Epoch: [2][ 300/3491]	Time  0.490 ( 1.406)	Loss 3.4456e-01 (3.3890e-01)
Epoch: [2][ 350/3491]	Time  0.489 ( 1.409)	Loss 3.1714e-01 (3.3799e-01)
Epoch: [2][ 400/3491]	Time  8.644 ( 1.430)	Loss 3.5766e-01 (3.3837e-01)
Epoch: [2][ 450/3491]	Time  0.488 ( 1.425)	Loss 3.2786e-01 (3.3865e-01)
Epoch: [2][ 500/3491]	Time  0.486 ( 1.427)	Loss 3.6469e-01 (3.3865e-01)
Epoch: [2][ 550/3491]	Time  0.490 ( 1.425)	Loss 3.1408e-01 (3.3910e-01)
Epoch: [2][ 600/3491]	Time  8.468 ( 1.441)	Loss 3.4488e-01 (3.3932e-01)
Epoch: [2][ 650/3491]	Time  0.486 ( 1.441)	Loss 3.6202e-01 (3.3899e-01)
Epoch: [2][ 700/3491]	Time  0.491 ( 1.443)	Loss 3.5381e-01 (3.3882e-01)
Epoch: [2][ 750/3491]	Time  0.486 ( 1.447)	Loss 3.3578e-01 (3.3890e-01)
Epoch: [2][ 800/3491]	Time  8.538 ( 1.460)	Loss 3.4568e-01 (3.3886e-01)
Epoch: [2][ 850/3491]	Time  0.485 ( 1.462)	Loss 3.1449e-01 (3.3879e-01)
Epoch: [2][ 900/3491]	Time  0.497 ( 1.467)	Loss 3.5507e-01 (3.3884e-01)
Epoch: [2][ 950/3491]	Time  0.491 ( 1.466)	Loss 2.9726e-01 (3.3899e-01)
Epoch: [2][1000/3491]	Time  8.103 ( 1.478)	Loss 3.7093e-01 (3.3906e-01)
Epoch: [2][1050/3491]	Time  0.490 ( 1.480)	Loss 3.1862e-01 (3.3897e-01)
Epoch: [2][1100/3491]	Time  0.487 ( 1.481)	Loss 3.4298e-01 (3.3884e-01)
Epoch: [2][1150/3491]	Time  0.486 ( 1.485)	Loss 3.2832e-01 (3.3901e-01)
Epoch: [2][1200/3491]	Time  2.427 ( 1.490)	Loss 3.6437e-01 (3.3905e-01)
Epoch: [2][1250/3491]	Time  0.486 ( 1.494)	Loss 3.4244e-01 (3.3893e-01)
Epoch: [2][1300/3491]	Time  0.485 ( 1.499)	Loss 3.3868e-01 (3.3896e-01)
Epoch: [2][1350/3491]	Time  0.489 ( 1.502)	Loss 3.2048e-01 (3.3898e-01)
Epoch: [2][1400/3491]	Time  0.490 ( 1.505)	Loss 3.1107e-01 (3.3888e-01)
Epoch: [2][1450/3491]	Time  0.490 ( 1.505)	Loss 3.6895e-01 (3.3903e-01)
Epoch: [2][1500/3491]	Time  0.490 ( 1.513)	Loss 3.1476e-01 (3.3901e-01)
Epoch: [2][1550/3491]	Time  0.486 ( 1.512)	Loss 3.3547e-01 (3.3911e-01)
Epoch: [2][1600/3491]	Time  0.486 ( 1.512)	Loss 3.6293e-01 (3.3905e-01)
Epoch: [2][1650/3491]	Time  0.486 ( 1.511)	Loss 3.6852e-01 (3.3898e-01)
Epoch: [2][1700/3491]	Time  0.485 ( 1.516)	Loss 3.3664e-01 (3.3899e-01)
Epoch: [2][1750/3491]	Time  0.486 ( 1.515)	Loss 3.8970e-01 (3.3901e-01)
Epoch: [2][1800/3491]	Time  0.486 ( 1.515)	Loss 3.3645e-01 (3.3915e-01)
Epoch: [2][1850/3491]	Time  0.486 ( 1.516)	Loss 3.3375e-01 (3.3912e-01)
Epoch: [2][1900/3491]	Time  0.486 ( 1.522)	Loss 3.3528e-01 (3.3914e-01)
Epoch: [2][1950/3491]	Time  0.486 ( 1.522)	Loss 3.3203e-01 (3.3919e-01)
Epoch: [2][2000/3491]	Time  0.488 ( 1.525)	Loss 3.0105e-01 (3.3916e-01)
Epoch: [2][2050/3491]	Time  0.491 ( 1.525)	Loss 3.4900e-01 (3.3920e-01)
Epoch: [2][2100/3491]	Time  0.490 ( 1.531)	Loss 3.3748e-01 (3.3913e-01)
Epoch: [2][2150/3491]	Time  0.489 ( 1.531)	Loss 3.5509e-01 (3.3908e-01)
Epoch: [2][2200/3491]	Time  0.490 ( 1.533)	Loss 3.4116e-01 (3.3904e-01)
Epoch: [2][2250/3491]	Time  0.489 ( 1.533)	Loss 3.3832e-01 (3.3906e-01)
Epoch: [2][2300/3491]	Time  0.486 ( 1.539)	Loss 3.5722e-01 (3.3898e-01)
Epoch: [2][2350/3491]	Time  0.486 ( 1.541)	Loss 3.5083e-01 (3.3889e-01)
Epoch: [2][2400/3491]	Time  0.486 ( 1.542)	Loss 3.6656e-01 (3.3892e-01)
Epoch: [2][2450/3491]	Time  0.488 ( 1.543)	Loss 3.2657e-01 (3.3896e-01)
Epoch: [2][2500/3491]	Time  0.488 ( 1.549)	Loss 3.5034e-01 (3.3899e-01)
Epoch: [2][2550/3491]	Time  0.490 ( 1.550)	Loss 3.4268e-01 (3.3888e-01)
Epoch: [2][2600/3491]	Time  0.486 ( 1.551)	Loss 3.4338e-01 (3.3894e-01)
Epoch: [2][2650/3491]	Time  0.486 ( 1.552)	Loss 3.3673e-01 (3.3885e-01)
Epoch: [2][2700/3491]	Time  0.486 ( 1.555)	Loss 3.4396e-01 (3.3881e-01)
Epoch: [2][2750/3491]	Time  0.486 ( 1.554)	Loss 3.2998e-01 (3.3882e-01)
Epoch: [2][2800/3491]	Time  0.490 ( 1.554)	Loss 3.3768e-01 (3.3877e-01)
Epoch: [2][2850/3491]	Time  0.489 ( 1.553)	Loss 3.5915e-01 (3.3875e-01)
Epoch: [2][2900/3491]	Time  0.491 ( 1.557)	Loss 3.3497e-01 (3.3869e-01)
Epoch: [2][2950/3491]	Time  0.489 ( 1.555)	Loss 3.6645e-01 (3.3867e-01)
Epoch: [2][3000/3491]	Time  0.491 ( 1.554)	Loss 3.0493e-01 (3.3854e-01)
Epoch: [2][3050/3491]	Time  0.489 ( 1.555)	Loss 3.4494e-01 (3.3854e-01)
Epoch: [2][3100/3491]	Time  0.491 ( 1.558)	Loss 2.9332e-01 (3.3852e-01)
Epoch: [2][3150/3491]	Time  0.486 ( 1.558)	Loss 3.7906e-01 (3.3852e-01)
Epoch: [2][3200/3491]	Time  0.486 ( 1.558)	Loss 3.4993e-01 (3.3854e-01)
Epoch: [2][3250/3491]	Time  0.486 ( 1.557)	Loss 3.1440e-01 (3.3854e-01)
Epoch: [2][3300/3491]	Time  0.486 ( 1.560)	Loss 3.0515e-01 (3.3860e-01)
Epoch: [2][3350/3491]	Time  0.489 ( 1.562)	Loss 3.4853e-01 (3.3858e-01)
Epoch: [2][3400/3491]	Time  0.488 ( 1.562)	Loss 3.4298e-01 (3.3857e-01)
Epoch: [2][3450/3491]	Time  0.501 ( 1.562)	Loss 3.3734e-01 (3.3860e-01)
Val: [0/4]	Time 12.091 (12.091)	Loss 2.9309e-01 (2.9309e-01)
Epoch 0002: val_loss did not improve from 0.33484 
Epoch: [3][   0/3491]	Time 11.837 (11.837)	Loss 3.4539e-01 (3.4539e-01)
Epoch: [3][  50/3491]	Time  2.185 ( 1.518)	Loss 3.1414e-01 (3.3667e-01)
Epoch: [3][ 100/3491]	Time  0.489 ( 1.457)	Loss 3.4208e-01 (3.3647e-01)
Epoch: [3][ 150/3491]	Time  0.486 ( 1.450)	Loss 3.3794e-01 (3.3687e-01)
Epoch: [3][ 200/3491]	Time  0.489 ( 1.441)	Loss 3.2389e-01 (3.3691e-01)
Epoch: [3][ 250/3491]	Time  9.380 ( 1.468)	Loss 3.6517e-01 (3.3695e-01)
Epoch: [3][ 300/3491]	Time  0.486 ( 1.469)	Loss 3.2064e-01 (3.3724e-01)
Epoch: [3][ 350/3491]	Time  0.485 ( 1.472)	Loss 3.4756e-01 (3.3694e-01)
Epoch: [3][ 400/3491]	Time  0.486 ( 1.462)	Loss 3.3500e-01 (3.3651e-01)
Epoch: [3][ 450/3491]	Time  7.260 ( 1.473)	Loss 3.3696e-01 (3.3637e-01)
Epoch: [3][ 500/3491]	Time  0.489 ( 1.477)	Loss 3.2694e-01 (3.3624e-01)
Epoch: [3][ 550/3491]	Time  0.489 ( 1.471)	Loss 3.3688e-01 (3.3605e-01)
Epoch: [3][ 600/3491]	Time  0.491 ( 1.469)	Loss 3.2004e-01 (3.3606e-01)
Epoch: [3][ 650/3491]	Time  8.307 ( 1.477)	Loss 3.6686e-01 (3.3626e-01)
Epoch: [3][ 700/3491]	Time  0.489 ( 1.486)	Loss 3.1691e-01 (3.3623e-01)
Epoch: [3][ 750/3491]	Time  0.489 ( 1.488)	Loss 3.2544e-01 (3.3597e-01)
Epoch: [3][ 800/3491]	Time  0.490 ( 1.489)	Loss 3.3622e-01 (3.3574e-01)
Epoch: [3][ 850/3491]	Time  8.653 ( 1.497)	Loss 3.3994e-01 (3.3580e-01)
Epoch: [3][ 900/3491]	Time  0.485 ( 1.504)	Loss 3.4875e-01 (3.3583e-01)
Epoch: [3][ 950/3491]	Time  0.486 ( 1.505)	Loss 2.9261e-01 (3.3597e-01)
Epoch: [3][1000/3491]	Time  0.486 ( 1.507)	Loss 3.0944e-01 (3.3615e-01)
Epoch: [3][1050/3491]	Time  8.363 ( 1.514)	Loss 3.3721e-01 (3.3634e-01)
Epoch: [3][1100/3491]	Time  0.489 ( 1.512)	Loss 3.5640e-01 (3.3631e-01)
Epoch: [3][1150/3491]	Time  0.489 ( 1.509)	Loss 3.2816e-01 (3.3628e-01)
Epoch: [3][1200/3491]	Time  0.490 ( 1.509)	Loss 3.2013e-01 (3.3621e-01)
Epoch: [3][1250/3491]	Time  0.514 ( 1.507)	Loss 3.2979e-01 (3.3608e-01)
Epoch: [3][1300/3491]	Time  0.491 ( 1.506)	Loss 2.9897e-01 (3.3607e-01)
Epoch: [3][1350/3491]	Time  0.490 ( 1.510)	Loss 3.2225e-01 (3.3600e-01)
Epoch: [3][1400/3491]	Time  0.490 ( 1.509)	Loss 3.4798e-01 (3.3597e-01)
Epoch: [3][1450/3491]	Time  0.490 ( 1.509)	Loss 3.5723e-01 (3.3583e-01)
Epoch: [3][1500/3491]	Time  0.489 ( 1.509)	Loss 3.1621e-01 (3.3577e-01)
Epoch: [3][1550/3491]	Time  0.490 ( 1.520)	Loss 3.2066e-01 (3.3561e-01)
Epoch: [3][1600/3491]	Time  0.490 ( 1.537)	Loss 3.4710e-01 (3.3571e-01)
Epoch: [3][1650/3491]	Time  0.488 ( 1.545)	Loss 3.1996e-01 (3.3564e-01)
Epoch: [3][1700/3491]	Time  0.486 ( 1.550)	Loss 3.6936e-01 (3.3570e-01)
Epoch: [3][1750/3491]	Time  0.488 ( 1.559)	Loss 3.1481e-01 (3.3575e-01)
Epoch: [3][1800/3491]	Time  3.387 ( 1.567)	Loss 3.4298e-01 (3.3576e-01)
Epoch: [3][1850/3491]	Time  0.490 ( 1.572)	Loss 3.0526e-01 (3.3569e-01)
Epoch: [3][1900/3491]	Time  0.490 ( 1.579)	Loss 3.6030e-01 (3.3573e-01)
Epoch: [3][1950/3491]	Time  0.486 ( 1.590)	Loss 3.1767e-01 (3.3573e-01)
Epoch: [3][2000/3491]	Time  0.490 ( 1.605)	Loss 3.2781e-01 (3.3572e-01)
Epoch: [3][2050/3491]	Time  0.491 ( 1.610)	Loss 3.5295e-01 (3.3568e-01)
Epoch: [3][2100/3491]	Time  0.489 ( 1.617)	Loss 3.1615e-01 (3.3578e-01)
Epoch: [3][2150/3491]	Time  0.490 ( 1.623)	Loss 3.3608e-01 (3.3586e-01)
Epoch: [3][2200/3491]	Time  0.488 ( 1.630)	Loss 3.4469e-01 (3.3585e-01)
Epoch: [3][2250/3491]	Time  0.488 ( 1.635)	Loss 3.5534e-01 (3.3590e-01)
Epoch: [3][2300/3491]	Time  0.490 ( 1.638)	Loss 3.7148e-01 (3.3589e-01)
Epoch: [3][2350/3491]	Time  0.491 ( 1.641)	Loss 3.2402e-01 (3.3586e-01)
Epoch: [3][2400/3491]	Time  0.490 ( 1.647)	Loss 3.3846e-01 (3.3593e-01)
Epoch: [3][2450/3491]	Time  0.489 ( 1.647)	Loss 3.2278e-01 (3.3593e-01)
Epoch: [3][2500/3491]	Time  0.491 ( 1.645)	Loss 3.1318e-01 (3.3594e-01)
Epoch: [3][2550/3491]	Time  0.490 ( 1.645)	Loss 3.6731e-01 (3.3593e-01)
Epoch: [3][2600/3491]	Time  0.486 ( 1.647)	Loss 3.5487e-01 (3.3584e-01)
Epoch: [3][2650/3491]	Time  0.490 ( 1.645)	Loss 3.4045e-01 (3.3582e-01)
Epoch: [3][2700/3491]	Time  0.490 ( 1.644)	Loss 3.2439e-01 (3.3573e-01)
Epoch: [3][2750/3491]	Time  0.486 ( 1.642)	Loss 3.0466e-01 (3.3570e-01)
Epoch: [3][2800/3491]	Time  0.490 ( 1.641)	Loss 3.0178e-01 (3.3567e-01)
Epoch: [3][2850/3491]	Time  0.490 ( 1.638)	Loss 3.2161e-01 (3.3570e-01)
Epoch: [3][2900/3491]	Time  0.490 ( 1.636)	Loss 3.5600e-01 (3.3575e-01)
Epoch: [3][2950/3491]	Time  3.338 ( 1.634)	Loss 3.4280e-01 (3.3565e-01)
Epoch: [3][3000/3491]	Time  0.490 ( 1.633)	Loss 3.8057e-01 (3.3564e-01)
Epoch: [3][3050/3491]	Time  0.489 ( 1.635)	Loss 3.3891e-01 (3.3571e-01)
Epoch: [3][3100/3491]	Time  0.490 ( 1.634)	Loss 3.2722e-01 (3.3566e-01)
Epoch: [3][3150/3491]	Time  9.416 ( 1.636)	Loss 3.2880e-01 (3.3563e-01)
Epoch: [3][3200/3491]	Time  0.490 ( 1.634)	Loss 3.3821e-01 (3.3565e-01)
Epoch: [3][3250/3491]	Time  0.489 ( 1.633)	Loss 3.0674e-01 (3.3567e-01)
Epoch: [3][3300/3491]	Time  0.486 ( 1.632)	Loss 3.3203e-01 (3.3560e-01)
Epoch: [3][3350/3491]	Time  5.353 ( 1.632)	Loss 3.4749e-01 (3.3555e-01)
Epoch: [3][3400/3491]	Time  0.591 ( 1.633)	Loss 3.2445e-01 (3.3553e-01)
Epoch: [3][3450/3491]	Time  0.489 ( 1.633)	Loss 3.1290e-01 (3.3552e-01)
Val: [0/4]	Time 14.060 (14.060)	Loss 2.8819e-01 (2.8819e-01)
Epoch 0003: val_loss did not improve from 0.33484 
Epoch: [4][   0/3491]	Time 13.060 (13.060)	Loss 3.3340e-01 (3.3340e-01)
Epoch: [4][  50/3491]	Time  0.489 ( 1.599)	Loss 3.0804e-01 (3.3912e-01)
Epoch: [4][ 100/3491]	Time  1.042 ( 1.540)	Loss 3.3996e-01 (3.3410e-01)
Epoch: [4][ 150/3491]	Time  0.489 ( 1.505)	Loss 3.3936e-01 (3.3370e-01)
Epoch: [4][ 200/3491]	Time  2.003 ( 1.484)	Loss 3.0752e-01 (3.3364e-01)
Epoch: [4][ 250/3491]	Time  0.490 ( 1.457)	Loss 3.4448e-01 (3.3397e-01)
Epoch: [4][ 300/3491]	Time  4.689 ( 1.472)	Loss 3.3462e-01 (3.3365e-01)
Epoch: [4][ 350/3491]	Time  0.488 ( 1.459)	Loss 3.2608e-01 (3.3353e-01)
Epoch: [4][ 400/3491]	Time  0.491 ( 1.456)	Loss 3.7965e-01 (3.3311e-01)
Epoch: [4][ 450/3491]	Time  0.491 ( 1.447)	Loss 3.4138e-01 (3.3341e-01)
Epoch: [4][ 500/3491]	Time  0.489 ( 1.449)	Loss 3.0566e-01 (3.3332e-01)
Epoch: [4][ 550/3491]	Time  0.490 ( 1.446)	Loss 3.2313e-01 (3.3333e-01)
Epoch: [4][ 600/3491]	Time  0.490 ( 1.443)	Loss 3.0778e-01 (3.3340e-01)
Epoch: [4][ 650/3491]	Time  1.395 ( 1.438)	Loss 3.3200e-01 (3.3316e-01)
Epoch: [4][ 700/3491]	Time  0.490 ( 1.443)	Loss 3.6588e-01 (3.3306e-01)
Epoch: [4][ 750/3491]	Time  0.490 ( 1.440)	Loss 3.1113e-01 (3.3322e-01)
Epoch: [4][ 800/3491]	Time  0.490 ( 1.437)	Loss 3.6213e-01 (3.3327e-01)
Epoch: [4][ 850/3491]	Time  3.093 ( 1.451)	Loss 3.4706e-01 (3.3333e-01)
Epoch: [4][ 900/3491]	Time  3.476 ( 1.457)	Loss 3.5710e-01 (3.3336e-01)
Epoch: [4][ 950/3491]	Time  0.489 ( 1.451)	Loss 3.5295e-01 (3.3353e-01)
Epoch: [4][1000/3491]	Time  0.490 ( 1.448)	Loss 3.3636e-01 (3.3345e-01)
Epoch: [4][1050/3491]	Time  7.952 ( 1.460)	Loss 3.2565e-01 (3.3353e-01)
Epoch: [4][1100/3491]	Time  1.589 ( 1.460)	Loss 3.4858e-01 (3.3353e-01)
Epoch: [4][1150/3491]	Time  0.491 ( 1.458)	Loss 3.6857e-01 (3.3343e-01)
Epoch: [4][1200/3491]	Time  0.490 ( 1.456)	Loss 3.3154e-01 (3.3365e-01)
Epoch: [4][1250/3491]	Time  6.047 ( 1.458)	Loss 3.1216e-01 (3.3369e-01)
Epoch: [4][1300/3491]	Time  2.307 ( 1.458)	Loss 3.2004e-01 (3.3350e-01)
Epoch: [4][1350/3491]	Time  0.489 ( 1.455)	Loss 3.5926e-01 (3.3343e-01)
Epoch: [4][1400/3491]	Time  0.489 ( 1.454)	Loss 3.2825e-01 (3.3340e-01)
Epoch: [4][1450/3491]	Time  2.712 ( 1.454)	Loss 3.4930e-01 (3.3335e-01)
Epoch: [4][1500/3491]	Time  3.461 ( 1.457)	Loss 3.2824e-01 (3.3337e-01)
Epoch: [4][1550/3491]	Time  0.490 ( 1.460)	Loss 3.1180e-01 (3.3320e-01)
Epoch: [4][1600/3491]	Time  0.491 ( 1.462)	Loss 3.3351e-01 (3.3331e-01)
Epoch: [4][1650/3491]	Time  6.915 ( 1.464)	Loss 3.0470e-01 (3.3331e-01)
Epoch: [4][1700/3491]	Time  4.862 ( 1.465)	Loss 3.3279e-01 (3.3336e-01)
Epoch: [4][1750/3491]	Time  0.486 ( 1.464)	Loss 3.7406e-01 (3.3349e-01)
Epoch: [4][1800/3491]	Time  0.490 ( 1.463)	Loss 3.1419e-01 (3.3349e-01)
Epoch: [4][1850/3491]	Time  0.910 ( 1.461)	Loss 3.2824e-01 (3.3339e-01)
Epoch: [4][1900/3491]	Time  6.626 ( 1.464)	Loss 3.3650e-01 (3.3332e-01)
Epoch: [4][1950/3491]	Time  0.488 ( 1.462)	Loss 3.1247e-01 (3.3317e-01)
Epoch: [4][2000/3491]	Time  0.490 ( 1.462)	Loss 2.9756e-01 (3.3319e-01)
Epoch: [4][2050/3491]	Time  2.979 ( 1.462)	Loss 3.7883e-01 (3.3319e-01)
Epoch: [4][2100/3491]	Time  4.829 ( 1.463)	Loss 3.0786e-01 (3.3320e-01)
Epoch: [4][2150/3491]	Time  0.491 ( 1.461)	Loss 3.6387e-01 (3.3319e-01)
Epoch: [4][2200/3491]	Time  0.490 ( 1.460)	Loss 3.0347e-01 (3.3313e-01)
Epoch: [4][2250/3491]	Time  3.451 ( 1.459)	Loss 3.2439e-01 (3.3305e-01)
Epoch: [4][2300/3491]	Time  8.784 ( 1.460)	Loss 3.3297e-01 (3.3303e-01)
Epoch: [4][2350/3491]	Time  0.490 ( 1.457)	Loss 3.0533e-01 (3.3308e-01)
Epoch: [4][2400/3491]	Time  0.490 ( 1.456)	Loss 3.2565e-01 (3.3308e-01)
Epoch: [4][2450/3491]	Time  0.490 ( 1.456)	Loss 3.2602e-01 (3.3309e-01)
Epoch: [4][2500/3491]	Time  8.390 ( 1.458)	Loss 2.9579e-01 (3.3300e-01)
Epoch: [4][2550/3491]	Time  0.486 ( 1.456)	Loss 3.5938e-01 (3.3294e-01)
Epoch: [4][2600/3491]	Time  0.488 ( 1.457)	Loss 3.1736e-01 (3.3287e-01)
Epoch: [4][2650/3491]	Time  0.491 ( 1.456)	Loss 3.3044e-01 (3.3284e-01)
Epoch: [4][2700/3491]	Time  7.980 ( 1.460)	Loss 3.4468e-01 (3.3276e-01)
Epoch: [4][2750/3491]	Time  0.490 ( 1.461)	Loss 3.2215e-01 (3.3281e-01)
Epoch: [4][2800/3491]	Time  0.489 ( 1.461)	Loss 3.2401e-01 (3.3275e-01)
Epoch: [4][2850/3491]	Time  0.491 ( 1.460)	Loss 3.0525e-01 (3.3271e-01)
Epoch: [4][2900/3491]	Time  7.740 ( 1.462)	Loss 3.3783e-01 (3.3270e-01)
Epoch: [4][2950/3491]	Time  0.491 ( 1.462)	Loss 3.3941e-01 (3.3274e-01)
Epoch: [4][3000/3491]	Time  0.491 ( 1.461)	Loss 3.5714e-01 (3.3274e-01)
Epoch: [4][3050/3491]	Time  0.489 ( 1.460)	Loss 3.4126e-01 (3.3272e-01)
Epoch: [4][3100/3491]	Time  7.687 ( 1.462)	Loss 3.2705e-01 (3.3273e-01)
Epoch: [4][3150/3491]	Time  0.486 ( 1.461)	Loss 3.1018e-01 (3.3278e-01)
Epoch: [4][3200/3491]	Time  0.489 ( 1.461)	Loss 3.5690e-01 (3.3277e-01)
Epoch: [4][3250/3491]	Time  0.488 ( 1.461)	Loss 3.0627e-01 (3.3278e-01)
Epoch: [4][3300/3491]	Time  8.501 ( 1.463)	Loss 3.2879e-01 (3.3283e-01)
Epoch: [4][3350/3491]	Time  0.489 ( 1.461)	Loss 3.5662e-01 (3.3280e-01)
Epoch: [4][3400/3491]	Time  0.490 ( 1.461)	Loss 3.5922e-01 (3.3280e-01)
Epoch: [4][3450/3491]	Time  0.490 ( 1.461)	Loss 2.9452e-01 (3.3280e-01)
Val: [0/4]	Time 11.573 (11.573)	Loss 2.9835e-01 (2.9835e-01)
Epoch 0004: val_loss did not improve from 0.33484 
Epoch: [5][   0/3491]	Time 11.155 (11.155)	Loss 3.2092e-01 (3.2092e-01)
Epoch: [5][  50/3491]	Time  0.485 ( 1.488)	Loss 3.6221e-01 (3.3089e-01)
Epoch: [5][ 100/3491]	Time  0.490 ( 1.370)	Loss 3.7177e-01 (3.3090e-01)
Epoch: [5][ 150/3491]	Time  0.486 ( 1.301)	Loss 3.2044e-01 (3.3089e-01)
Epoch: [5][ 200/3491]	Time  5.914 ( 1.293)	Loss 3.3141e-01 (3.3090e-01)
Epoch: [5][ 250/3491]	Time  0.490 ( 1.280)	Loss 3.0653e-01 (3.3073e-01)
Epoch: [5][ 300/3491]	Time  0.488 ( 1.257)	Loss 3.3572e-01 (3.3081e-01)
Epoch: [5][ 350/3491]	Time  0.490 ( 1.247)	Loss 3.2065e-01 (3.3089e-01)
Epoch: [5][ 400/3491]	Time  5.230 ( 1.252)	Loss 3.4737e-01 (3.3105e-01)
Epoch: [5][ 450/3491]	Time  3.699 ( 1.260)	Loss 3.3543e-01 (3.3064e-01)
Epoch: [5][ 500/3491]	Time  0.582 ( 1.264)	Loss 3.2580e-01 (3.3068e-01)
Epoch: [5][ 550/3491]	Time  0.489 ( 1.262)	Loss 2.9936e-01 (3.3073e-01)
Epoch: [5][ 600/3491]	Time  8.483 ( 1.274)	Loss 3.0473e-01 (3.3063e-01)
Epoch: [5][ 650/3491]	Time  0.491 ( 1.278)	Loss 3.2192e-01 (3.3105e-01)
Epoch: [5][ 700/3491]	Time  0.490 ( 1.283)	Loss 3.4089e-01 (3.3082e-01)
Epoch: [5][ 750/3491]	Time  0.489 ( 1.295)	Loss 3.4015e-01 (3.3083e-01)
Epoch: [5][ 800/3491]	Time  9.134 ( 1.317)	Loss 3.2369e-01 (3.3092e-01)
Epoch: [5][ 850/3491]	Time  0.486 ( 1.335)	Loss 3.4972e-01 (3.3086e-01)
Epoch: [5][ 900/3491]	Time  0.488 ( 1.350)	Loss 3.1573e-01 (3.3102e-01)
Epoch: [5][ 950/3491]	Time  0.490 ( 1.365)	Loss 3.4875e-01 (3.3108e-01)
Epoch: [5][1000/3491]	Time  8.929 ( 1.385)	Loss 3.4943e-01 (3.3106e-01)
Epoch: [5][1050/3491]	Time  0.489 ( 1.398)	Loss 3.1471e-01 (3.3107e-01)
Epoch: [5][1100/3491]	Time  0.486 ( 1.406)	Loss 3.2421e-01 (3.3117e-01)
Epoch: [5][1150/3491]	Time  0.490 ( 1.415)	Loss 3.5232e-01 (3.3118e-01)
Epoch: [5][1200/3491]	Time  9.947 ( 1.430)	Loss 3.3933e-01 (3.3131e-01)
Epoch: [5][1250/3491]	Time  0.490 ( 1.441)	Loss 3.1772e-01 (3.3126e-01)
Epoch: [5][1300/3491]	Time  0.490 ( 1.451)	Loss 3.0448e-01 (3.3141e-01)
Epoch: [5][1350/3491]	Time  0.491 ( 1.461)	Loss 3.1360e-01 (3.3157e-01)
Epoch: [5][1400/3491]	Time 10.165 ( 1.479)	Loss 3.2600e-01 (3.3156e-01)
Epoch: [5][1450/3491]	Time  0.491 ( 1.486)	Loss 3.0118e-01 (3.3139e-01)
Epoch: [5][1500/3491]	Time  0.490 ( 1.491)	Loss 3.3447e-01 (3.3125e-01)
Epoch: [5][1550/3491]	Time  0.486 ( 1.494)	Loss 3.4098e-01 (3.3116e-01)
Epoch: [5][1600/3491]	Time 10.937 ( 1.507)	Loss 3.3463e-01 (3.3109e-01)
Epoch: [5][1650/3491]	Time  0.491 ( 1.511)	Loss 3.4578e-01 (3.3108e-01)
Epoch: [5][1700/3491]	Time  0.486 ( 1.517)	Loss 3.5373e-01 (3.3110e-01)
Epoch: [5][1750/3491]	Time  0.485 ( 1.520)	Loss 2.8890e-01 (3.3108e-01)
Epoch: [5][1800/3491]	Time 10.561 ( 1.531)	Loss 3.2833e-01 (3.3106e-01)
Epoch: [5][1850/3491]	Time  0.486 ( 1.536)	Loss 3.2884e-01 (3.3104e-01)
Epoch: [5][1900/3491]	Time  0.490 ( 1.544)	Loss 3.0821e-01 (3.3104e-01)
Epoch: [5][1950/3491]	Time  0.487 ( 1.550)	Loss 3.2265e-01 (3.3115e-01)
Epoch: [5][2000/3491]	Time  8.989 ( 1.557)	Loss 3.0836e-01 (3.3117e-01)
Epoch: [5][2050/3491]	Time  0.487 ( 1.555)	Loss 2.9631e-01 (3.3118e-01)
Epoch: [5][2100/3491]	Time  0.490 ( 1.557)	Loss 3.6540e-01 (3.3119e-01)
Epoch: [5][2150/3491]	Time  0.490 ( 1.557)	Loss 3.3126e-01 (3.3117e-01)
Epoch: [5][2200/3491]	Time  9.163 ( 1.559)	Loss 3.3213e-01 (3.3110e-01)
Epoch: [5][2250/3491]	Time  0.491 ( 1.557)	Loss 3.3831e-01 (3.3120e-01)
Epoch: [5][2300/3491]	Time  0.491 ( 1.554)	Loss 3.0060e-01 (3.3119e-01)
Epoch: [5][2350/3491]	Time  0.490 ( 1.553)	Loss 3.2932e-01 (3.3117e-01)
Epoch: [5][2400/3491]	Time  8.451 ( 1.555)	Loss 3.1032e-01 (3.3119e-01)
Epoch: [5][2450/3491]	Time  0.486 ( 1.553)	Loss 3.1814e-01 (3.3118e-01)
Epoch: [5][2500/3491]	Time  0.486 ( 1.552)	Loss 3.2599e-01 (3.3112e-01)
Epoch: [5][2550/3491]	Time  0.490 ( 1.552)	Loss 3.4503e-01 (3.3106e-01)
Epoch: [5][2600/3491]	Time  9.854 ( 1.555)	Loss 3.1979e-01 (3.3101e-01)
Epoch: [5][2650/3491]	Time  0.491 ( 1.555)	Loss 3.1618e-01 (3.3102e-01)
Epoch: [5][2700/3491]	Time  0.490 ( 1.553)	Loss 3.3392e-01 (3.3095e-01)
Epoch: [5][2750/3491]	Time  0.491 ( 1.552)	Loss 3.4722e-01 (3.3095e-01)
Epoch: [5][2800/3491]	Time  8.580 ( 1.554)	Loss 3.2627e-01 (3.3096e-01)
Epoch: [5][2850/3491]	Time  0.490 ( 1.553)	Loss 2.9655e-01 (3.3089e-01)
Epoch: [5][2900/3491]	Time  0.485 ( 1.552)	Loss 3.3519e-01 (3.3084e-01)
Epoch: [5][2950/3491]	Time  0.486 ( 1.551)	Loss 3.4991e-01 (3.3098e-01)
Epoch: [5][3000/3491]	Time  8.869 ( 1.553)	Loss 3.2036e-01 (3.3091e-01)
Epoch: [5][3050/3491]	Time  0.485 ( 1.551)	Loss 3.3380e-01 (3.3097e-01)
Epoch: [5][3100/3491]	Time  0.486 ( 1.549)	Loss 3.4738e-01 (3.3100e-01)
Epoch: [5][3150/3491]	Time  0.490 ( 1.547)	Loss 2.9759e-01 (3.3103e-01)
Epoch: [5][3200/3491]	Time  8.405 ( 1.549)	Loss 3.0174e-01 (3.3098e-01)
Epoch: [5][3250/3491]	Time  0.486 ( 1.547)	Loss 3.0551e-01 (3.3098e-01)
Epoch: [5][3300/3491]	Time  0.491 ( 1.546)	Loss 3.3881e-01 (3.3099e-01)
Epoch: [5][3350/3491]	Time  0.491 ( 1.544)	Loss 3.2691e-01 (3.3097e-01)
Epoch: [5][3400/3491]	Time  8.678 ( 1.545)	Loss 3.4671e-01 (3.3098e-01)
Epoch: [5][3450/3491]	Time  0.486 ( 1.546)	Loss 3.1997e-01 (3.3108e-01)
Val: [0/4]	Time 12.066 (12.066)	Loss 2.8077e-01 (2.8077e-01)
Epoch 0005: val_loss did not improve from 0.33484 
Epoch: [6][   0/3491]	Time 10.290 (10.290)	Loss 3.3343e-01 (3.3343e-01)
Epoch: [6][  50/3491]	Time  0.487 ( 1.403)	Loss 3.4945e-01 (3.2937e-01)
Epoch: [6][ 100/3491]	Time  0.486 ( 1.308)	Loss 3.0268e-01 (3.2746e-01)
Epoch: [6][ 150/3491]	Time  0.486 ( 1.301)	Loss 3.0127e-01 (3.2809e-01)
Epoch: [6][ 200/3491]	Time  2.886 ( 1.293)	Loss 3.0153e-01 (3.2922e-01)
Epoch: [6][ 250/3491]	Time  2.051 ( 1.284)	Loss 3.2800e-01 (3.2949e-01)
Epoch: [6][ 300/3491]	Time  0.488 ( 1.281)	Loss 3.1341e-01 (3.2973e-01)
Epoch: [6][ 350/3491]	Time  1.862 ( 1.279)	Loss 3.1963e-01 (3.2991e-01)
Epoch: [6][ 400/3491]	Time  0.486 ( 1.291)	Loss 3.0761e-01 (3.3013e-01)
Epoch: [6][ 450/3491]	Time  1.145 ( 1.291)	Loss 3.2928e-01 (3.2976e-01)
Epoch: [6][ 500/3491]	Time  0.486 ( 1.306)	Loss 3.5408e-01 (3.3014e-01)
Epoch: [6][ 550/3491]	Time  0.885 ( 1.307)	Loss 3.7735e-01 (3.3008e-01)
Epoch: [6][ 600/3491]	Time  0.486 ( 1.322)	Loss 3.1312e-01 (3.3009e-01)
Epoch: [6][ 650/3491]	Time  0.488 ( 1.320)	Loss 3.5836e-01 (3.3028e-01)
Epoch: [6][ 700/3491]	Time  0.491 ( 1.316)	Loss 3.2033e-01 (3.2986e-01)
Epoch: [6][ 750/3491]	Time  1.193 ( 1.312)	Loss 3.2573e-01 (3.3011e-01)
Epoch: [6][ 800/3491]	Time  0.489 ( 1.317)	Loss 3.2768e-01 (3.2987e-01)
Epoch: [6][ 850/3491]	Time  0.490 ( 1.313)	Loss 2.9276e-01 (3.2962e-01)
Epoch: [6][ 900/3491]	Time  1.168 ( 1.312)	Loss 3.1872e-01 (3.2965e-01)
Epoch: [6][ 950/3491]	Time  0.489 ( 1.311)	Loss 3.2118e-01 (3.2966e-01)
Epoch: [6][1000/3491]	Time  0.489 ( 1.314)	Loss 3.2347e-01 (3.2950e-01)
Epoch: [6][1050/3491]	Time  4.094 ( 1.318)	Loss 3.3597e-01 (3.2944e-01)
Epoch: [6][1100/3491]	Time  5.535 ( 1.320)	Loss 3.1729e-01 (3.2946e-01)
Epoch: [6][1150/3491]	Time  0.490 ( 1.319)	Loss 3.4810e-01 (3.2941e-01)
Epoch: [6][1200/3491]	Time  0.486 ( 1.319)	Loss 3.6712e-01 (3.2951e-01)
Epoch: [6][1250/3491]	Time  0.486 ( 1.320)	Loss 3.2984e-01 (3.2951e-01)
Epoch: [6][1300/3491]	Time  5.984 ( 1.323)	Loss 3.5898e-01 (3.2954e-01)
Epoch: [6][1350/3491]	Time  0.486 ( 1.324)	Loss 3.2122e-01 (3.2948e-01)
Epoch: [6][1400/3491]	Time  0.486 ( 1.325)	Loss 3.2969e-01 (3.2934e-01)
Epoch: [6][1450/3491]	Time  0.486 ( 1.329)	Loss 3.4812e-01 (3.2922e-01)
Epoch: [6][1500/3491]	Time  8.490 ( 1.336)	Loss 3.2180e-01 (3.2928e-01)
Epoch: [6][1550/3491]	Time  0.491 ( 1.339)	Loss 3.0834e-01 (3.2923e-01)
Epoch: [6][1600/3491]	Time  0.490 ( 1.340)	Loss 3.5266e-01 (3.2924e-01)
Epoch: [6][1650/3491]	Time  0.488 ( 1.341)	Loss 3.3819e-01 (3.2943e-01)
Epoch: [6][1700/3491]	Time  6.803 ( 1.344)	Loss 3.0884e-01 (3.2947e-01)
Epoch: [6][1750/3491]	Time  0.486 ( 1.346)	Loss 3.5252e-01 (3.2934e-01)
Epoch: [6][1800/3491]	Time  0.486 ( 1.348)	Loss 3.5052e-01 (3.2951e-01)
Epoch: [6][1850/3491]	Time  0.486 ( 1.348)	Loss 3.2869e-01 (3.2952e-01)
Epoch: [6][1900/3491]	Time  7.651 ( 1.353)	Loss 3.3784e-01 (3.2956e-01)
Epoch: [6][1950/3491]	Time  0.486 ( 1.353)	Loss 3.1660e-01 (3.2950e-01)
Epoch: [6][2000/3491]	Time  0.488 ( 1.354)	Loss 3.3118e-01 (3.2947e-01)
Epoch: [6][2050/3491]	Time  0.486 ( 1.353)	Loss 3.5672e-01 (3.2941e-01)
Epoch: [6][2100/3491]	Time  7.574 ( 1.356)	Loss 2.9193e-01 (3.2942e-01)
Epoch: [6][2150/3491]	Time  0.489 ( 1.358)	Loss 3.2769e-01 (3.2935e-01)
Epoch: [6][2200/3491]	Time  0.488 ( 1.359)	Loss 3.1013e-01 (3.2942e-01)
Epoch: [6][2250/3491]	Time  0.491 ( 1.361)	Loss 3.5770e-01 (3.2942e-01)
Epoch: [6][2300/3491]	Time  7.050 ( 1.364)	Loss 3.4101e-01 (3.2954e-01)
Epoch: [6][2350/3491]	Time  0.486 ( 1.365)	Loss 3.0188e-01 (3.2954e-01)
Epoch: [6][2400/3491]	Time  0.486 ( 1.366)	Loss 3.2659e-01 (3.2951e-01)
Epoch: [6][2450/3491]	Time  0.485 ( 1.370)	Loss 3.1293e-01 (3.2950e-01)
Epoch: [6][2500/3491]	Time  9.407 ( 1.376)	Loss 3.2650e-01 (3.2951e-01)
Epoch: [6][2550/3491]	Time  0.486 ( 1.376)	Loss 3.4658e-01 (3.2948e-01)
Epoch: [6][2600/3491]	Time  0.486 ( 1.378)	Loss 3.4210e-01 (3.2945e-01)
Epoch: [6][2650/3491]	Time  0.486 ( 1.378)	Loss 3.4685e-01 (3.2954e-01)
Epoch: [6][2700/3491]	Time  7.813 ( 1.380)	Loss 3.3413e-01 (3.2961e-01)
Epoch: [6][2750/3491]	Time  0.490 ( 1.381)	Loss 3.3311e-01 (3.2957e-01)
Epoch: [6][2800/3491]	Time  0.490 ( 1.381)	Loss 3.4022e-01 (3.2958e-01)
Epoch: [6][2850/3491]	Time  0.490 ( 1.380)	Loss 3.1206e-01 (3.2958e-01)
Epoch: [6][2900/3491]	Time  7.551 ( 1.382)	Loss 3.3040e-01 (3.2956e-01)
Epoch: [6][2950/3491]	Time  0.488 ( 1.380)	Loss 3.1841e-01 (3.2957e-01)
Epoch: [6][3000/3491]	Time  0.486 ( 1.380)	Loss 2.8020e-01 (3.2952e-01)
Epoch: [6][3050/3491]	Time  0.486 ( 1.383)	Loss 3.0575e-01 (3.2940e-01)
Epoch: [6][3100/3491]	Time  8.987 ( 1.388)	Loss 3.1510e-01 (3.2938e-01)
Epoch: [6][3150/3491]	Time  0.486 ( 1.392)	Loss 3.6228e-01 (3.2938e-01)
Epoch: [6][3200/3491]	Time  0.486 ( 1.395)	Loss 3.3340e-01 (3.2944e-01)
Epoch: [6][3250/3491]	Time  0.486 ( 1.396)	Loss 3.2928e-01 (3.2940e-01)
Epoch: [6][3300/3491]	Time  8.076 ( 1.400)	Loss 3.3244e-01 (3.2940e-01)
Epoch: [6][3350/3491]	Time  0.488 ( 1.400)	Loss 3.2480e-01 (3.2940e-01)
Epoch: [6][3400/3491]	Time  0.491 ( 1.401)	Loss 3.3612e-01 (3.2938e-01)
Epoch: [6][3450/3491]	Time  0.491 ( 1.403)	Loss 3.4090e-01 (3.2940e-01)
Val: [0/4]	Time 10.955 (10.955)	Loss 2.8733e-01 (2.8733e-01)
Epoch 0006: val_loss did not improve from 0.33484 
Epoch: [7][   0/3491]	Time  9.179 ( 9.179)	Loss 2.9367e-01 (2.9367e-01)
Epoch: [7][  50/3491]	Time  0.490 ( 1.237)	Loss 3.3129e-01 (3.3222e-01)
Epoch: [7][ 100/3491]	Time  1.072 ( 1.194)	Loss 3.0238e-01 (3.3063e-01)
Epoch: [7][ 150/3491]	Time  0.487 ( 1.197)	Loss 3.2113e-01 (3.2890e-01)
Epoch: [7][ 200/3491]	Time  2.364 ( 1.225)	Loss 3.1064e-01 (3.2782e-01)
Epoch: [7][ 250/3491]	Time  0.485 ( 1.271)	Loss 3.1616e-01 (3.2702e-01)
Epoch: [7][ 300/3491]	Time  0.487 ( 1.267)	Loss 2.9476e-01 (3.2834e-01)
Epoch: [7][ 350/3491]	Time  0.488 ( 1.254)	Loss 3.3137e-01 (3.2828e-01)
Epoch: [7][ 400/3491]	Time  3.461 ( 1.265)	Loss 3.2973e-01 (3.2843e-01)
Epoch: [7][ 450/3491]	Time  0.490 ( 1.265)	Loss 3.4827e-01 (3.2862e-01)
Epoch: [7][ 500/3491]	Time  0.491 ( 1.256)	Loss 3.1253e-01 (3.2798e-01)
Epoch: [7][ 550/3491]	Time  0.486 ( 1.255)	Loss 2.8896e-01 (3.2841e-01)
Epoch: [7][ 600/3491]	Time  2.726 ( 1.256)	Loss 3.1051e-01 (3.2799e-01)
Epoch: [7][ 650/3491]	Time  0.486 ( 1.261)	Loss 3.1539e-01 (3.2796e-01)
Epoch: [7][ 700/3491]	Time  0.487 ( 1.267)	Loss 3.0529e-01 (3.2802e-01)
Epoch: [7][ 750/3491]	Time  0.487 ( 1.278)	Loss 3.3792e-01 (3.2778e-01)
Epoch: [7][ 800/3491]	Time  0.486 ( 1.282)	Loss 3.2939e-01 (3.2802e-01)
Epoch: [7][ 850/3491]	Time  0.490 ( 1.284)	Loss 3.5777e-01 (3.2812e-01)
Epoch: [7][ 900/3491]	Time  0.491 ( 1.285)	Loss 3.0656e-01 (3.2798e-01)
Epoch: [7][ 950/3491]	Time  0.491 ( 1.291)	Loss 3.6069e-01 (3.2814e-01)
Epoch: [7][1000/3491]	Time  0.490 ( 1.293)	Loss 3.0832e-01 (3.2822e-01)
Epoch: [7][1050/3491]	Time  0.491 ( 1.292)	Loss 3.3425e-01 (3.2808e-01)
Epoch: [7][1100/3491]	Time  0.490 ( 1.293)	Loss 3.3630e-01 (3.2807e-01)
Epoch: [7][1150/3491]	Time  0.489 ( 1.302)	Loss 3.5002e-01 (3.2802e-01)
Epoch: [7][1200/3491]	Time  0.486 ( 1.304)	Loss 3.3146e-01 (3.2788e-01)
Epoch: [7][1250/3491]	Time  0.486 ( 1.307)	Loss 3.3017e-01 (3.2792e-01)
Epoch: [7][1300/3491]	Time  0.486 ( 1.310)	Loss 3.1505e-01 (3.2792e-01)
Epoch: [7][1350/3491]	Time  0.491 ( 1.322)	Loss 3.3483e-01 (3.2789e-01)
Epoch: [7][1400/3491]	Time  0.490 ( 1.325)	Loss 2.8774e-01 (3.2808e-01)
Epoch: [7][1450/3491]	Time  0.491 ( 1.328)	Loss 3.3223e-01 (3.2803e-01)
Epoch: [7][1500/3491]	Time  0.488 ( 1.330)	Loss 3.1050e-01 (3.2801e-01)
Epoch: [7][1550/3491]	Time  0.487 ( 1.336)	Loss 3.2186e-01 (3.2789e-01)
Epoch: [7][1600/3491]	Time  0.486 ( 1.339)	Loss 3.1302e-01 (3.2787e-01)
Epoch: [7][1650/3491]	Time  0.486 ( 1.340)	Loss 3.1925e-01 (3.2788e-01)
Epoch: [7][1700/3491]	Time  0.486 ( 1.343)	Loss 3.1307e-01 (3.2776e-01)
Epoch: [7][1750/3491]	Time  0.487 ( 1.349)	Loss 3.1685e-01 (3.2769e-01)
Epoch: [7][1800/3491]	Time  0.486 ( 1.349)	Loss 3.2067e-01 (3.2769e-01)
Epoch: [7][1850/3491]	Time  0.486 ( 1.350)	Loss 3.0164e-01 (3.2760e-01)
Epoch: [7][1900/3491]	Time  0.488 ( 1.352)	Loss 3.3007e-01 (3.2747e-01)
Epoch: [7][1950/3491]	Time  0.491 ( 1.358)	Loss 3.5564e-01 (3.2740e-01)
Epoch: [7][2000/3491]	Time  0.491 ( 1.361)	Loss 3.3092e-01 (3.2740e-01)
Epoch: [7][2050/3491]	Time  0.490 ( 1.366)	Loss 3.4232e-01 (3.2735e-01)
Epoch: [7][2100/3491]	Time  0.487 ( 1.368)	Loss 3.5090e-01 (3.2733e-01)
Epoch: [7][2150/3491]	Time  0.487 ( 1.375)	Loss 3.3698e-01 (3.2739e-01)
Epoch: [7][2200/3491]	Time  0.486 ( 1.377)	Loss 3.4349e-01 (3.2737e-01)
Epoch: [7][2250/3491]	Time  0.486 ( 1.380)	Loss 3.4823e-01 (3.2738e-01)
Epoch: [7][2300/3491]	Time  0.490 ( 1.384)	Loss 3.7224e-01 (3.2743e-01)
Epoch: [7][2350/3491]	Time  0.491 ( 1.392)	Loss 2.8662e-01 (3.2741e-01)
Epoch: [7][2400/3491]	Time  0.489 ( 1.394)	Loss 2.9866e-01 (3.2749e-01)
Epoch: [7][2450/3491]	Time  0.491 ( 1.397)	Loss 3.5834e-01 (3.2756e-01)
Epoch: [7][2500/3491]	Time  0.491 ( 1.399)	Loss 3.2792e-01 (3.2754e-01)
Epoch: [7][2550/3491]	Time  0.490 ( 1.403)	Loss 3.2653e-01 (3.2745e-01)
Epoch: [7][2600/3491]	Time  0.486 ( 1.403)	Loss 3.0454e-01 (3.2743e-01)
Epoch: [7][2650/3491]	Time  0.488 ( 1.405)	Loss 2.7763e-01 (3.2745e-01)
Epoch: [7][2700/3491]	Time  0.490 ( 1.404)	Loss 3.2460e-01 (3.2752e-01)
Epoch: [7][2750/3491]	Time  0.486 ( 1.406)	Loss 3.2010e-01 (3.2752e-01)
Epoch: [7][2800/3491]	Time  0.487 ( 1.408)	Loss 3.2640e-01 (3.2758e-01)
Epoch: [7][2850/3491]	Time  0.486 ( 1.410)	Loss 3.0537e-01 (3.2761e-01)
Epoch: [7][2900/3491]	Time  0.486 ( 1.410)	Loss 3.4533e-01 (3.2761e-01)
Epoch: [7][2950/3491]	Time  0.486 ( 1.412)	Loss 3.2118e-01 (3.2763e-01)
Epoch: [7][3000/3491]	Time  0.486 ( 1.412)	Loss 3.5833e-01 (3.2764e-01)
Epoch: [7][3050/3491]	Time  0.488 ( 1.411)	Loss 3.0841e-01 (3.2773e-01)
Epoch: [7][3100/3491]	Time  0.488 ( 1.411)	Loss 3.3742e-01 (3.2772e-01)
Epoch: [7][3150/3491]	Time  0.489 ( 1.411)	Loss 3.0238e-01 (3.2768e-01)
Epoch: [7][3200/3491]	Time  0.486 ( 1.413)	Loss 3.5077e-01 (3.2771e-01)
Epoch: [7][3250/3491]	Time  0.486 ( 1.411)	Loss 2.9434e-01 (3.2774e-01)
Epoch: [7][3300/3491]	Time  0.486 ( 1.411)	Loss 3.0574e-01 (3.2775e-01)
Epoch: [7][3350/3491]	Time  0.487 ( 1.410)	Loss 3.2267e-01 (3.2778e-01)
Epoch: [7][3400/3491]	Time  0.486 ( 1.412)	Loss 3.0199e-01 (3.2776e-01)
Epoch: [7][3450/3491]	Time  0.485 ( 1.411)	Loss 3.3973e-01 (3.2779e-01)
Val: [0/4]	Time 11.455 (11.455)	Loss 2.8542e-01 (2.8542e-01)
Epoch 0007: val_loss did not improve from 0.33484 
start testing.....
>> Disease = ['No Finding', 'Enlarged Cardiomediastinum', 'Cardiomegaly', 'Lung Opacity', 'Lung Lesion', 'Edema', 'Consolidation', 'Pneumonia', 'Atelectasis', 'Pneumothorax', 'Pleural Effusion', 'Pleural Other', 'Fracture', 'Support Devices']
Creating model...
Creating model from pretrained weights: /home/nyarava/ARK/Ark/ark6_teacher_ep200_swinb_projector1376_mlp.pth.tar
Removing key head.weight from pretrained checkpoint
Removing key head.bias from pretrained checkpoint
Loaded with msg: _IncompatibleKeys(missing_keys=['head.weight', 'head.bias'], unexpected_keys=['projector.0.weight', 'projector.0.bias', 'projector.2.weight', 'projector.2.bias', 'omni_heads.0.weight', 'omni_heads.0.bias', 'omni_heads.1.weight', 'omni_heads.1.bias', 'omni_heads.2.weight', 'omni_heads.2.bias', 'omni_heads.3.weight', 'omni_heads.3.bias', 'omni_heads.4.weight', 'omni_heads.4.bias', 'omni_heads.5.weight', 'omni_heads.5.bias'])
SwinTransformer(
  (patch_embed): PatchEmbed(
    (proj): Conv2d(3, 128, kernel_size=(4, 4), stride=(4, 4))
    (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
  )
  (pos_drop): Dropout(p=0.0, inplace=False)
  (layers): Sequential(
    (0): BasicLayer(
      dim=128, input_resolution=(56, 56), depth=2
      (blocks): ModuleList(
        (0): SwinTransformerBlock(
          (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            (qkv): Linear(in_features=128, out_features=384, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=128, out_features=128, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): Identity()
          (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=128, out_features=512, bias=True)
            (act): GELU(approximate='none')
            (drop1): Dropout(p=0.0, inplace=False)
            (fc2): Linear(in_features=512, out_features=128, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
        )
        (1): SwinTransformerBlock(
          (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            (qkv): Linear(in_features=128, out_features=384, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=128, out_features=128, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=128, out_features=512, bias=True)
            (act): GELU(approximate='none')
            (drop1): Dropout(p=0.0, inplace=False)
            (fc2): Linear(in_features=512, out_features=128, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (downsample): PatchMerging(
        input_resolution=(56, 56), dim=128
        (reduction): Linear(in_features=512, out_features=256, bias=False)
        (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
    )
    (1): BasicLayer(
      dim=256, input_resolution=(28, 28), depth=2
      (blocks): ModuleList(
        (0-1): 2 x SwinTransformerBlock(
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            (qkv): Linear(in_features=256, out_features=768, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=256, out_features=1024, bias=True)
            (act): GELU(approximate='none')
            (drop1): Dropout(p=0.0, inplace=False)
            (fc2): Linear(in_features=1024, out_features=256, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (downsample): PatchMerging(
        input_resolution=(28, 28), dim=256
        (reduction): Linear(in_features=1024, out_features=512, bias=False)
        (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
    )
    (2): BasicLayer(
      dim=512, input_resolution=(14, 14), depth=18
      (blocks): ModuleList(
        (0-17): 18 x SwinTransformerBlock(
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU(approximate='none')
            (drop1): Dropout(p=0.0, inplace=False)
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (downsample): PatchMerging(
        input_resolution=(14, 14), dim=512
        (reduction): Linear(in_features=2048, out_features=1024, bias=False)
        (norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
    )
    (3): BasicLayer(
      dim=1024, input_resolution=(7, 7), depth=2
      (blocks): ModuleList(
        (0-1): 2 x SwinTransformerBlock(
          (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            (qkv): Linear(in_features=1024, out_features=3072, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=1024, out_features=1024, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=1024, out_features=4096, bias=True)
            (act): GELU(approximate='none')
            (drop1): Dropout(p=0.0, inplace=False)
            (fc2): Linear(in_features=4096, out_features=1024, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
        )
      )
    )
  )
  (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
  (avgpool): AdaptiveAvgPool1d(output_size=1)
  (head): Linear(in_features=1024, out_features=14, bias=True)
)
=> loaded pre-trained model './Models/Classification/CheXpert/swin_base_ark/swin_base_ark_run_0.pth.tar'
  0%|          | 0/11 [00:00<?, ?it/s]  9%|▉         | 1/11 [00:14<02:21, 14.14s/it] 18%|█▊        | 2/11 [00:15<00:59,  6.65s/it] 27%|██▋       | 3/11 [00:16<00:34,  4.26s/it] 36%|███▋      | 4/11 [00:18<00:21,  3.13s/it] 45%|████▌     | 5/11 [00:19<00:15,  2.51s/it] 55%|█████▍    | 6/11 [00:21<00:10,  2.13s/it] 64%|██████▎   | 7/11 [00:22<00:07,  1.89s/it] 73%|███████▎  | 8/11 [00:23<00:05,  1.74s/it] 82%|████████▏ | 9/11 [00:26<00:04,  2.00s/it] 91%|█████████ | 10/11 [00:27<00:01,  1.82s/it]100%|██████████| 11/11 [00:29<00:00,  1.71s/it]100%|██████████| 11/11 [00:29<00:00,  2.69s/it]
>>swin_base_ark_run_0: AUC = [0.8853,0.495 ,0.8856,0.9407,0.8446,0.929 ,0.8928,0.8038,0.8711,0.9745,
 0.9614,0.8763,0.6475,0.9695]
>>swin_base_ark_run_0: AUC = 0.8555
Creating model...
Creating model from pretrained weights: /home/nyarava/ARK/Ark/ark6_teacher_ep200_swinb_projector1376_mlp.pth.tar
Removing key head.weight from pretrained checkpoint
Removing key head.bias from pretrained checkpoint
Loaded with msg: _IncompatibleKeys(missing_keys=['head.weight', 'head.bias'], unexpected_keys=['projector.0.weight', 'projector.0.bias', 'projector.2.weight', 'projector.2.bias', 'omni_heads.0.weight', 'omni_heads.0.bias', 'omni_heads.1.weight', 'omni_heads.1.bias', 'omni_heads.2.weight', 'omni_heads.2.bias', 'omni_heads.3.weight', 'omni_heads.3.bias', 'omni_heads.4.weight', 'omni_heads.4.bias', 'omni_heads.5.weight', 'omni_heads.5.bias'])
SwinTransformer(
  (patch_embed): PatchEmbed(
    (proj): Conv2d(3, 128, kernel_size=(4, 4), stride=(4, 4))
    (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
  )
  (pos_drop): Dropout(p=0.0, inplace=False)
  (layers): Sequential(
    (0): BasicLayer(
      dim=128, input_resolution=(56, 56), depth=2
      (blocks): ModuleList(
        (0): SwinTransformerBlock(
          (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            (qkv): Linear(in_features=128, out_features=384, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=128, out_features=128, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): Identity()
          (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=128, out_features=512, bias=True)
            (act): GELU(approximate='none')
            (drop1): Dropout(p=0.0, inplace=False)
            (fc2): Linear(in_features=512, out_features=128, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
        )
        (1): SwinTransformerBlock(
          (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            (qkv): Linear(in_features=128, out_features=384, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=128, out_features=128, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=128, out_features=512, bias=True)
            (act): GELU(approximate='none')
            (drop1): Dropout(p=0.0, inplace=False)
            (fc2): Linear(in_features=512, out_features=128, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (downsample): PatchMerging(
        input_resolution=(56, 56), dim=128
        (reduction): Linear(in_features=512, out_features=256, bias=False)
        (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
    )
    (1): BasicLayer(
      dim=256, input_resolution=(28, 28), depth=2
      (blocks): ModuleList(
        (0-1): 2 x SwinTransformerBlock(
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            (qkv): Linear(in_features=256, out_features=768, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=256, out_features=1024, bias=True)
            (act): GELU(approximate='none')
            (drop1): Dropout(p=0.0, inplace=False)
            (fc2): Linear(in_features=1024, out_features=256, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (downsample): PatchMerging(
        input_resolution=(28, 28), dim=256
        (reduction): Linear(in_features=1024, out_features=512, bias=False)
        (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
    )
    (2): BasicLayer(
      dim=512, input_resolution=(14, 14), depth=18
      (blocks): ModuleList(
        (0-17): 18 x SwinTransformerBlock(
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU(approximate='none')
            (drop1): Dropout(p=0.0, inplace=False)
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (downsample): PatchMerging(
        input_resolution=(14, 14), dim=512
        (reduction): Linear(in_features=2048, out_features=1024, bias=False)
        (norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
    )
    (3): BasicLayer(
      dim=1024, input_resolution=(7, 7), depth=2
      (blocks): ModuleList(
        (0-1): 2 x SwinTransformerBlock(
          (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            (qkv): Linear(in_features=1024, out_features=3072, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=1024, out_features=1024, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=1024, out_features=4096, bias=True)
            (act): GELU(approximate='none')
            (drop1): Dropout(p=0.0, inplace=False)
            (fc2): Linear(in_features=4096, out_features=1024, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
        )
      )
    )
  )
  (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
  (avgpool): AdaptiveAvgPool1d(output_size=1)
  (head): Linear(in_features=1024, out_features=14, bias=True)
)
=> loaded pre-trained model './Models/Classification/CheXpert/swin_base_ark/swin_base_ark_run_0.pth.tar'
  0%|          | 0/11 [00:00<?, ?it/s]  9%|▉         | 1/11 [00:05<00:59,  5.91s/it] 18%|█▊        | 2/11 [00:07<00:29,  3.26s/it] 27%|██▋       | 3/11 [00:08<00:19,  2.42s/it] 36%|███▋      | 4/11 [00:10<00:14,  2.02s/it] 45%|████▌     | 5/11 [00:11<00:10,  1.80s/it] 55%|█████▍    | 6/11 [00:12<00:08,  1.67s/it] 64%|██████▎   | 7/11 [00:14<00:06,  1.58s/it] 73%|███████▎  | 8/11 [00:15<00:04,  1.53s/it] 82%|████████▏ | 9/11 [00:17<00:02,  1.49s/it] 91%|█████████ | 10/11 [00:18<00:01,  1.47s/it]100%|██████████| 11/11 [00:19<00:00,  1.44s/it]100%|██████████| 11/11 [00:20<00:00,  1.83s/it]
>>swin_base_ark_run_0: AUC = [0.8853,0.495 ,0.8856,0.9407,0.8446,0.929 ,0.8928,0.8038,0.8711,0.9745,
 0.9614,0.8763,0.6475,0.9695]
>>swin_base_ark_run_0: AUC = 0.8555
>> All trials: mAUC  = [0.8555,0.8555]
>> Mean AUC over All trials: = 0.8555
>> STD over All trials:  = 0.0000
