/home/nyarava/miniconda3/envs/yolo/lib/python3.12/site-packages/torch/functional.py:507: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3549.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
{'GPU': None, 'model_name': 'swin_base', 'init': 'ark', 'pretrained_weights': '/home/nyarava/ARK/Ark/ark6_teacher_ep200_swinb_projector1376_mlp.pth.tar', 'num_class': 14, 'data_set': 'CheXpert', 'normalization': 'imagenet', 'img_size': 224, 'img_depth': 3, 'data_dir': '/scratch/nyarava/hey.zip/chexpertchestxrays-u20210408/', 'train_list': '/scratch/nyarava/hey.zip/chexpertchestxrays-u20210408/CheXpert-v1.0/train.csv', 'val_list': '/scratch/nyarava/hey.zip/chexpertchestxrays-u20210408/CheXpert-v1.0/valid.csv', 'test_list': '/scratch/nyarava/hey.zip/chexpertchestxrays-u20210408/CheXpert-v1.0/test_labels.csv', 'mode': 'train', 'batch_size': 64, 'epochs': 8, 'exp_name': '', 'opt': 'adamw', 'opt_eps': 1e-08, 'opt_betas': None, 'clip_grad': None, 'momentum': 0.9, 'weight_decay': 0.0001, 'sched': 'cosine', 'lr': 0.001, 'lr_noise': None, 'lr_noise_pct': 0.67, 'lr_noise_std': 1.0, 'warmup_lr': 1e-06, 'min_lr': 1e-05, 'decay_epochs': 30, 'warmup_epochs': 3, 'cooldown_epochs': 10, 'decay_rate': 0.5, 'patience': 10, 'early_stop': True, 'num_trial': 1, 'start_index': 0, 'clean': False, 'resume': False, 'workers': 8, 'print_freq': 50, 'test_augment': True, 'anno_percent': 100, 'device': 'cuda', 'activate': 'Sigmoid', 'uncertain_label': 'LSR-Ones', 'unknown_label': 0}
start training....
run: 1
Creating model...
Creating model from pretrained weights: /home/nyarava/ARK/Ark/ark6_teacher_ep200_swinb_projector1376_mlp.pth.tar
Removing key head.weight from pretrained checkpoint
Removing key head.bias from pretrained checkpoint
Loaded with msg: _IncompatibleKeys(missing_keys=['head.weight', 'head.bias'], unexpected_keys=['projector.0.weight', 'projector.0.bias', 'projector.2.weight', 'projector.2.bias', 'omni_heads.0.weight', 'omni_heads.0.bias', 'omni_heads.1.weight', 'omni_heads.1.bias', 'omni_heads.2.weight', 'omni_heads.2.bias', 'omni_heads.3.weight', 'omni_heads.3.bias', 'omni_heads.4.weight', 'omni_heads.4.bias', 'omni_heads.5.weight', 'omni_heads.5.bias'])
SwinTransformer(
  (patch_embed): PatchEmbed(
    (proj): Conv2d(3, 128, kernel_size=(4, 4), stride=(4, 4))
    (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
  )
  (pos_drop): Dropout(p=0.0, inplace=False)
  (layers): Sequential(
    (0): BasicLayer(
      dim=128, input_resolution=(56, 56), depth=2
      (blocks): ModuleList(
        (0): SwinTransformerBlock(
          (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            (qkv): Linear(in_features=128, out_features=384, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=128, out_features=128, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): Identity()
          (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=128, out_features=512, bias=True)
            (act): GELU(approximate='none')
            (drop1): Dropout(p=0.0, inplace=False)
            (fc2): Linear(in_features=512, out_features=128, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
        )
        (1): SwinTransformerBlock(
          (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            (qkv): Linear(in_features=128, out_features=384, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=128, out_features=128, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=128, out_features=512, bias=True)
            (act): GELU(approximate='none')
            (drop1): Dropout(p=0.0, inplace=False)
            (fc2): Linear(in_features=512, out_features=128, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (downsample): PatchMerging(
        input_resolution=(56, 56), dim=128
        (reduction): Linear(in_features=512, out_features=256, bias=False)
        (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      )
    )
    (1): BasicLayer(
      dim=256, input_resolution=(28, 28), depth=2
      (blocks): ModuleList(
        (0-1): 2 x SwinTransformerBlock(
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            (qkv): Linear(in_features=256, out_features=768, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=256, out_features=256, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=256, out_features=1024, bias=True)
            (act): GELU(approximate='none')
            (drop1): Dropout(p=0.0, inplace=False)
            (fc2): Linear(in_features=1024, out_features=256, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (downsample): PatchMerging(
        input_resolution=(28, 28), dim=256
        (reduction): Linear(in_features=1024, out_features=512, bias=False)
        (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
      )
    )
    (2): BasicLayer(
      dim=512, input_resolution=(14, 14), depth=18
      (blocks): ModuleList(
        (0-17): 18 x SwinTransformerBlock(
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            (qkv): Linear(in_features=512, out_features=1536, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=512, out_features=512, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU(approximate='none')
            (drop1): Dropout(p=0.0, inplace=False)
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (downsample): PatchMerging(
        input_resolution=(14, 14), dim=512
        (reduction): Linear(in_features=2048, out_features=1024, bias=False)
        (norm): LayerNorm((2048,), eps=1e-05, elementwise_affine=True)
      )
    )
    (3): BasicLayer(
      dim=1024, input_resolution=(7, 7), depth=2
      (blocks): ModuleList(
        (0-1): 2 x SwinTransformerBlock(
          (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            (qkv): Linear(in_features=1024, out_features=3072, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=1024, out_features=1024, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=1024, out_features=4096, bias=True)
            (act): GELU(approximate='none')
            (drop1): Dropout(p=0.0, inplace=False)
            (fc2): Linear(in_features=4096, out_features=1024, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
        )
      )
    )
  )
  (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
  (avgpool): AdaptiveAvgPool1d(output_size=1)
  (head): Linear(in_features=1024, out_features=14, bias=True)
)
Epoch: [0][   0/3491]	Time 26.717 (26.717)	Loss 6.6962e-01 (6.6962e-01)
Epoch: [0][  50/3491]	Time  0.489 ( 2.221)	Loss 5.4555e-01 (5.9115e-01)
Epoch: [0][ 100/3491]	Time  0.489 ( 1.995)	Loss 4.5568e-01 (5.3526e-01)
Epoch: [0][ 150/3491]	Time  0.488 ( 1.907)	Loss 4.1486e-01 (4.9808e-01)
Epoch: [0][ 200/3491]	Time  5.969 ( 1.882)	Loss 4.0123e-01 (4.7426e-01)
Epoch: [0][ 250/3491]	Time  0.485 ( 1.869)	Loss 3.8941e-01 (4.5793e-01)
Epoch: [0][ 300/3491]	Time  0.487 ( 1.847)	Loss 3.5734e-01 (4.4565e-01)
Epoch: [0][ 350/3491]	Time  0.487 ( 1.828)	Loss 3.5768e-01 (4.3641e-01)
Epoch: [0][ 400/3491]	Time  0.488 ( 1.822)	Loss 3.6020e-01 (4.2942e-01)
Epoch: [0][ 450/3491]	Time  0.488 ( 1.840)	Loss 3.5881e-01 (4.2317e-01)
Epoch: [0][ 500/3491]	Time  0.486 ( 1.825)	Loss 3.2596e-01 (4.1791e-01)
Epoch: [0][ 550/3491]	Time  0.486 ( 1.816)	Loss 3.7946e-01 (4.1371e-01)
Epoch: [0][ 600/3491]	Time  0.489 ( 1.823)	Loss 3.5572e-01 (4.1025e-01)
Epoch: [0][ 650/3491]	Time  0.486 ( 1.812)	Loss 3.7202e-01 (4.0712e-01)
Epoch: [0][ 700/3491]	Time  0.488 ( 1.803)	Loss 3.7092e-01 (4.0413e-01)
Epoch: [0][ 750/3491]	Time  0.489 ( 1.796)	Loss 3.6481e-01 (4.0122e-01)
Epoch: [0][ 800/3491]	Time  0.489 ( 1.807)	Loss 3.3496e-01 (3.9854e-01)
Epoch: [0][ 850/3491]	Time  0.487 ( 1.804)	Loss 3.8708e-01 (3.9662e-01)
Epoch: [0][ 900/3491]	Time  0.488 ( 1.798)	Loss 3.4939e-01 (3.9439e-01)
Epoch: [0][ 950/3491]	Time  0.489 ( 1.796)	Loss 3.6667e-01 (3.9235e-01)
Epoch: [0][1000/3491]	Time  0.489 ( 1.804)	Loss 3.6123e-01 (3.9038e-01)
Epoch: [0][1050/3491]	Time  0.490 ( 1.801)	Loss 3.5409e-01 (3.8884e-01)
Epoch: [0][1100/3491]	Time  0.489 ( 1.805)	Loss 3.6705e-01 (3.8738e-01)
Epoch: [0][1150/3491]	Time  0.489 ( 1.812)	Loss 3.7457e-01 (3.8614e-01)
Epoch: [0][1200/3491]	Time  0.489 ( 1.829)	Loss 3.6750e-01 (3.8465e-01)
Epoch: [0][1250/3491]	Time  0.487 ( 1.826)	Loss 3.7301e-01 (3.8342e-01)
Epoch: [0][1300/3491]	Time  0.486 ( 1.820)	Loss 3.2675e-01 (3.8243e-01)
Epoch: [0][1350/3491]	Time  0.488 ( 1.815)	Loss 3.4558e-01 (3.8132e-01)
Epoch: [0][1400/3491]	Time  0.488 ( 1.820)	Loss 3.4143e-01 (3.8030e-01)
Epoch: [0][1450/3491]	Time  0.489 ( 1.821)	Loss 3.6987e-01 (3.7937e-01)
Epoch: [0][1500/3491]	Time  0.488 ( 1.824)	Loss 3.4270e-01 (3.7846e-01)
Epoch: [0][1550/3491]	Time  0.489 ( 1.822)	Loss 3.5262e-01 (3.7764e-01)
Epoch: [0][1600/3491]	Time  0.486 ( 1.829)	Loss 3.2872e-01 (3.7676e-01)
Epoch: [0][1650/3491]	Time  0.489 ( 1.829)	Loss 3.2723e-01 (3.7589e-01)
Epoch: [0][1700/3491]	Time  0.486 ( 1.833)	Loss 3.4507e-01 (3.7506e-01)
Epoch: [0][1750/3491]	Time  0.488 ( 1.833)	Loss 3.5404e-01 (3.7447e-01)
Epoch: [0][1800/3491]	Time  0.488 ( 1.839)	Loss 3.4424e-01 (3.7386e-01)
Epoch: [0][1850/3491]	Time  0.487 ( 1.837)	Loss 3.4264e-01 (3.7323e-01)
Epoch: [0][1900/3491]	Time  0.485 ( 1.834)	Loss 3.3102e-01 (3.7258e-01)
Epoch: [0][1950/3491]	Time  0.488 ( 1.831)	Loss 3.4695e-01 (3.7195e-01)
Epoch: [0][2000/3491]	Time  0.488 ( 1.831)	Loss 3.6055e-01 (3.7132e-01)
Epoch: [0][2050/3491]	Time  0.488 ( 1.826)	Loss 3.6502e-01 (3.7079e-01)
Epoch: [0][2100/3491]	Time  0.488 ( 1.823)	Loss 3.4062e-01 (3.7021e-01)
Epoch: [0][2150/3491]	Time  0.488 ( 1.822)	Loss 3.4455e-01 (3.6970e-01)
Epoch: [0][2200/3491]	Time  0.488 ( 1.822)	Loss 3.3632e-01 (3.6922e-01)
Epoch: [0][2250/3491]	Time  0.486 ( 1.820)	Loss 3.5027e-01 (3.6870e-01)
Epoch: [0][2300/3491]	Time  0.486 ( 1.815)	Loss 3.3759e-01 (3.6828e-01)
Epoch: [0][2350/3491]	Time  0.488 ( 1.811)	Loss 3.3222e-01 (3.6775e-01)
Epoch: [0][2400/3491]	Time  0.486 ( 1.811)	Loss 3.3619e-01 (3.6730e-01)
Epoch: [0][2450/3491]	Time  0.486 ( 1.808)	Loss 3.4384e-01 (3.6692e-01)
Epoch: [0][2500/3491]	Time  0.488 ( 1.805)	Loss 3.4543e-01 (3.6652e-01)
Epoch: [0][2550/3491]	Time  0.488 ( 1.802)	Loss 3.3954e-01 (3.6617e-01)
Epoch: [0][2600/3491]	Time  0.488 ( 1.802)	Loss 3.3720e-01 (3.6577e-01)
Epoch: [0][2650/3491]	Time  0.488 ( 1.799)	Loss 3.4998e-01 (3.6540e-01)
Epoch: [0][2700/3491]	Time  0.488 ( 1.796)	Loss 3.5422e-01 (3.6503e-01)
Epoch: [0][2750/3491]	Time  0.486 ( 1.794)	Loss 3.6888e-01 (3.6478e-01)
Epoch: [0][2800/3491]	Time  0.486 ( 1.795)	Loss 3.4642e-01 (3.6445e-01)
Epoch: [0][2850/3491]	Time  0.488 ( 1.792)	Loss 3.4147e-01 (3.6407e-01)
Epoch: [0][2900/3491]	Time  0.489 ( 1.789)	Loss 3.2598e-01 (3.6376e-01)
Epoch: [0][2950/3491]	Time  0.488 ( 1.785)	Loss 3.5413e-01 (3.6342e-01)
Epoch: [0][3000/3491]	Time  0.488 ( 1.786)	Loss 3.6686e-01 (3.6314e-01)
Epoch: [0][3050/3491]	Time  0.489 ( 1.783)	Loss 3.8187e-01 (3.6283e-01)
Epoch: [0][3100/3491]	Time  0.488 ( 1.779)	Loss 3.5172e-01 (3.6253e-01)
Epoch: [0][3150/3491]	Time  0.488 ( 1.777)	Loss 3.5276e-01 (3.6227e-01)
Epoch: [0][3200/3491]	Time  0.489 ( 1.779)	Loss 3.4105e-01 (3.6194e-01)
Epoch: [0][3250/3491]	Time  0.488 ( 1.777)	Loss 3.3259e-01 (3.6164e-01)
Epoch: [0][3300/3491]	Time  0.489 ( 1.776)	Loss 3.2721e-01 (3.6135e-01)
Epoch: [0][3350/3491]	Time  0.489 ( 1.774)	Loss 3.6715e-01 (3.6101e-01)
Epoch: [0][3400/3491]	Time  0.488 ( 1.774)	Loss 3.4419e-01 (3.6075e-01)
Epoch: [0][3450/3491]	Time  0.488 ( 1.773)	Loss 3.4886e-01 (3.6046e-01)
Val: [0/4]	Time 13.140 (13.140)	Loss 2.8311e-01 (2.8311e-01)
Epoch 0000: val_loss improved from 1000000.00000 to 0.34144, saving model to ./Models/Classification/CheXpert/swin_base_ark/swin_base_ark_run_0
Epoch 0000: Testing...
Traceback (most recent call last):
  File "/home/nyarava/ARK/Ark/BenchmarkTransformers/main_classification.py", line 206, in <module>
    main(args)
  File "/home/nyarava/ARK/Ark/BenchmarkTransformers/main_classification.py", line 161, in main
    classification_engine(args, model_path, output_path, diseases, dataset_train, dataset_val, dataset_test, test_diseases)
  File "/home/nyarava/ARK/Ark/BenchmarkTransformers/engine.py", line 289, in classification_engine
    test_results = evaluate_model(model, dataset_test, args)
                   ^^^^^^^^^^^^^^
NameError: name 'evaluate_model' is not defined
